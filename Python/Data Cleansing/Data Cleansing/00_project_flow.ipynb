{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/welcome.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primer día de trabajo\n",
    "\n",
    "Descubrimos que nuestra primera tarea será construir un modelo que sea capaz de **predecir el precio de una propiedad en el Estado de California** a partir de un conjunto de datos relativos a los distintos distritos (población, renta promedio...); cada registro está etiquetado con el precio promedio de la propiedad en ese distrito. El objetivo es que nuestro modelo pueda predecir el precio medio a partir del resto de features aprendiendo de los datos disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconociendo el escenario\n",
    "\n",
    "A bote pronto, la primera pregunta obvia es consultar con el jefe **¿Cuál es el propósito final del proyecto?**; seguramente la construcción de este modelo, sea un paso más para obtener un beneficio de algo. Esto condicionará:\n",
    "* El tipo de modelo que se escogerá.\n",
    "* La métrica para evaluar la precisión del modelo.\n",
    "* El tiempo dedicado a realizar ajustes sobre el mismo.\n",
    "\n",
    "También el jefe nos informa de que el resultado de nuestro algoritmo irá a parar a su vez (junto con otros datos) a otro modelo que determinará si es ventajoso o no invertir en una determinada zona, con el consiguiente efecto en la rentabilidad obtenida por la compañía.\n",
    "\n",
    "<img src=\"images/modelo.jpg\">\n",
    "\n",
    "Además es interesante saber **cómo se aborda el problema actualmente**, de este modo podremos tener una noción del rendimiento de la solución actual así como de algún insight adicional.\n",
    "\n",
    "Hablando con el jefe, este nos comenta de que la tarea actualmente se realiza de forma manual mediante un conjunto de reglas elaboradas por un conjunto de expertos; se trata de un proceso costoso en tiempo y esfuerzo y con una tasa de error de alrededor del 15%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinándonos\n",
    "\n",
    "A tenor de lo que se puede ver en el flujo de datos de arriba, no somos los únicos en este proceso que discurre desde los datos en bruto hasta las decisiones de inversión. Por tanto es conveniente mantener una conversación previa con los equipos que van a depender de nuestros datos para asegurarnos de que entendemos sus necesidades.\n",
    "\n",
    "Ciertamente podría darse el caso de que el sistema que reciba nuestros datos, esperara una clasificación categórica del tipo: *\"Muy Barato\"*, *\"Barato\"*, *\"En la media\"*, *\"Caro\"*, *\"Muy Caro\"* en vez de precios. Si esto fuera así, para nosotros no sería tan importante obtener el precio exacto.\n",
    "\n",
    "Supongamos que después de hablar con el equipo responsable, nos confirman que ellos necesitarán precios y no categorías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué tipo de tarea tenemos entre manos?,\n",
    "* Aprendizaje ¿*Supervisado* ó *No Supervisado*?\n",
    "* Si es Supervisado...¿Se trata de un problema de *Clasificación* o de *Regresión*?\n",
    "* ¿Deberíamos prepararlo para se *entrenado online o no*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANOS A LA OBRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Está compuesto por datos del censo de California de los años 90. Cada registro representa un *Block Group* que es la unidad geográfica mínima para la cual el censo de Estados Unidos publica datos muestrales y que comprende una población entre 600 y 3000 personas. Podemos llamar al Block Group, distrito. [Más información](http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obteniendo los datos\n",
    "\n",
    "En un entorno de trabajo, los datos estarían almacenados en alguna base de datos on premise o incluso en algún almacenamiento cloud (AWS, Azure, GCloud..); con lo que necesitaríamos unas credenciales de acceso así como familiarizarnos con el esquema de los datos. En este caso obtendremos los datos de Internet, (concretamente de [este](http://www.dcc.fc.up.pt/~ltorgo/) profesor de la Universidad de Porto) y elaboraremos una función para que, siempre que queramos, podamos obtener la última versión de los datos sin demasiadas manualidades (al final es también una excusa para ver unas cuantas librerías que nos pueden ser muy útiles en nuestro día a día).<br>\n",
    "**NOTA IMPORTANTE**: En ocasiones, la página de recursos de la Universidad de Porto del profesor Luis Torgo, no está habilitada. Hemos subido el dataset al github de datahack school para asegurar su disponibilidad. No obstante todo el crédito del mismo es del profesor Luis Torgo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por cuestiones didácticas, iremos cargando las distintas\n",
    "# bibliotecas y módulos, a medida que los vayamos necesitando.\n",
    "# En la vida real, todas las importaciones irían al principio\n",
    "# del código en orden alfabético (para facilitar su legibilidad).\n",
    "import os\n",
    "import tarfile\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **os** nos permitirá manejar y realizar diversas operaciones sobre nuestro sistema operativo: gestión de directorios y ficheros, ejecución de comandos, usuarios, permisos...\n",
    "* **tarfile** como su propio nombre indica, proporciona diversas funcionalidades para gestionar el empaquetado de ficheros. También es capaz de escribir y leer ficheros comprimidos mediante gzip y bz2.\n",
    "* **requests** ciertamente existen otras librerías capaz de manejar requests como `urllib` y `urllib2`; `requests` proporciona más funcionalidad y se ha convertido en el estándar de facto sobre como gestionar peticiones [http en Python](https://requests.readthedocs.io/en/master/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL donde se ubica nuestro fichero\n",
    "#HOUSING_URL = \"http://www.dcc.fc.up.pt/~ltorgo/Regression/\"\n",
    "HOUSING_GITHUB_URL = \"https://github.com/datahack-school/resources/raw/master/\"\n",
    "# Nombre de nuestro fichero\n",
    "HOUSING_TGZ_FILENAME = \"cal_housing.tgz\"\n",
    "# Path relativo de nuestro portátil donde queremos que se descargue el fichero\n",
    "HOUSING_LOCAL_PATH = \"datasets/housing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_housing_data(housing_url, housing_file, housing_path):\n",
    "    \"\"\"\n",
    "    Esta función permite obtener un determinado fichero .tar/.tar.gz/.targz ubicado en una\n",
    "    determinada URL, guardarlo en un directorio local de nuestra elección y \n",
    "    desarchivarlo allí mismo.\n",
    "    Argumentos:\n",
    "        - housing_url: url donde nuestro fichero está alojado (omitiendo el nombre del fichero).\n",
    "        - housing_file: nombre del fichero.\n",
    "        - housing_path: ubicación local donde se quiere descargar y desarchivar el fichero.\n",
    "    \"\"\"\n",
    "\n",
    "    # Comprueba que el directorio destino existe y si no créalo    \n",
    "    if not os.path.isdir(housing_path):\n",
    "        print(housing_path,\"does not exist, it will be created...\")\n",
    "        os.makedirs(housing_path)\n",
    "        print(housing_path,\"created!\")\n",
    "    tgz_path = os.path.join(housing_path, housing_file)\n",
    "    \n",
    "    header = {'User-Agent': 'Mozilla/5.0'}\n",
    "    # ¿Mozilla 5.0? para averiguar más sobre el User-agent https://webaim.org/blog/user-agent-string-history/\n",
    "    print(\"requesting housing dataset at \",housing_url)\n",
    "    # Hacemos una petición a la URL donde se ubican los datos\n",
    "    # stream=True permite mantener la conexion abierta y descargar el contenido poco a poco\n",
    "    response = requests.get(housing_url + housing_file, headers=header)\n",
    "    \n",
    "    # Si la request fue aceptada, volcaremos el contenido de la respuesta en nuestro disco (sobre los códigos\n",
    "    # de respuesta de una petición HTTP https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "    if(response.status_code == 200):\n",
    "        with open(tgz_path, 'wb') as handle:\n",
    "            for block in response.iter_content(1024):\n",
    "                handle.write(block)\n",
    "            print(\"download complete!\")\n",
    "            \n",
    "    # Lo siguiente es desempaquetar el contenido comprimido\n",
    "    print(\"Untarring files...\")\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    print(\"Extraction complete!\")\n",
    "    housing_tgz.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que la ejecución de esta función trata de acceder a una ubicación remota para descargar el dataset, asegúrate de tener conexión a Internet antes de invocar a la función que hemos definido. Si ya habías ejecutado la descarga antes y quieres reproducir otra vez los pasos de la descarga para comprobar su funcionamiento, borra el subdirectorio *housing* del directorio *datasets* (que encontrarás en el mismo directorio de este notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data(HOUSING_GITHUB_URL, HOUSING_TGZ_FILENAME, HOUSING_LOCAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultado de la ejecución de la función anterior, el fichero *cal_housing.tgz* debería haberse descargado a un path relativo al directorio de nuestro notebook (*./datasets/housing*) y además haberse desarchivado y descomprimido, quedando como resultado dos ficheros: uno con las cabeceras (*cal_housing.domain*) y otro con los datos (*cal_housing.data*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio donde se ubican las dos partes (.data y .domain) de nuestro fichero\n",
    "HOUSING_UNTAR_PATH = HOUSING_LOCAL_PATH + \"/CaliforniaHousing\"\n",
    "# Nombre de nuestro fichero destino (resultante de combinar el .data y el .domain)\n",
    "HOUSING_CONCAT_FILENAME = \"housing.csv\"\n",
    "# Ruta completa donde se generará el fichero destino\n",
    "HOUSING_TOTAL_PATH = HOUSING_LOCAL_PATH + \"/\" + HOUSING_CONCAT_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisemos primeramente el fichero cal_housing.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HOUSING_UNTAR_PATH + \"/cal_housing.domain\") as header_file:\n",
    "    print(header_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El fichero que contiene las cabeceras, no está precísamente formateado como a nosotros nos gustaría, es más, contiene un dato adicional además del nombre de la columna: su tipo. Como vemos todas las columnas son **\"continuous\"** lo cual nos indica que todas ellas tomarán valores numéricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se imprimen las cinco primeras líneas del fichero de datos para tener una noción de su aspecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lines = 5\n",
    "with open(HOUSING_UNTAR_PATH + \"/cal_housing.data\") as data_file:\n",
    "    # arange es un método de numpy que devuelve un array de valores\n",
    "    # que se corresponde con el intervalo [0,n_lines). Tanto el valor\n",
    "    # inicial, como el final, como el salto entre valores..son configurables.\n",
    "    for line in range(n_lines):\n",
    "        print(next(data_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenemos ahora ambos ficheros de manera que los nombres de los campos pasen a ser una única línea que constituya la cabecera del fichero de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **glob** esta librería aporta una funcionalidad similar al [*ls*](https://www.tecmint.com/use-wildcards-to-match-filenames-in-linux/) de UNIX, permitiendo el uso de expresiones regulares con respecto a ubicaciones absolutas y relativas para obtener una lista de los ficheros que se correspondan con aquellas. Vamos a utilizar concretamente el método `glob` de la librería `glob`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro objetivo ahora será obtener un único fichero a partir de los ficheros .domain y .data. Para ver el procedimiento, échale un vistazo a la diapositiva *00_project_Flow.ipynb (Concatenando ficheros)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_housing_data(housing_untar_path, housing_total_path):\n",
    "    \"\"\"\n",
    "    Esta función barrerá el directorio donde el tar descargado fue extraído, \n",
    "    procesará el fichero con la cabecera (.domain) dejando solo los nombres de las\n",
    "    features (sin su tipo) y lo volcará en un nuevo fichero seguido del contenido \n",
    "    del fichero que contiene el resto del total de los datos.\n",
    "    Argumentos:\n",
    "        - housing_untar_path: el directorio donde se encuentran los ficheros desempaquetados resultantes.\n",
    "        - housing_total_path: la ruta del fichero concatenación de los dos resultantes.    \n",
    "    \"\"\"\n",
    "    # glob.glob permitirá listar los ficheros contenidos en el directorio\n",
    "    # sorted permitirá ordenar los ficheros obtenidos mediante glob.glob según el criterio\n",
    "    # indicado, en este caso el tamaño (de menor a mayor tamaño).\n",
    "    concat_files = sorted(glob.glob(housing_untar_path + \"/*\"), key=os.path.getsize)\n",
    "    print(\"Merging files...\",concat_files)\n",
    "    with open(housing_total_path,\"w\") as outfile:\n",
    "        for part_file in concat_files:\n",
    "            with open(part_file, \"r\") as infile:\n",
    "                # El fichero con los nombres de los campos será aplanado de manera que al final quede una\n",
    "                # única línea: campo1, campo2, campo3, campo4...\n",
    "                if(\".domain\" in part_file):\n",
    "                    # Primero se leen todas sus líneas (el nombre de cada feature y su tipo)\n",
    "                    raw_header = infile.readlines()\n",
    "                    # Luego suprime el tipo (\": continous\") metiendo el resultado en una lista\n",
    "                    # LIST COMPRENHENSION\n",
    "                    header = [field.replace(\": continuous.\",\"\").strip() for field in raw_header]\n",
    "                    # Une cada elemento de la lista por \",\" y vuelca el resultado en el fichero de salida\n",
    "                    outfile.write(\",\".join(header)+\"\\n\")\n",
    "                else:\n",
    "                    outfile.write(infile.read())   \n",
    "    print(\"Files merged into\",outfile.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_housing_data(HOUSING_UNTAR_PATH, HOUSING_TOTAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Ya tenemos nuestro fichero listo para comenzar a trabajar sobre el!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toma de contacto con los datos\n",
    "\n",
    "Cargaremos el dataset en una de las estructuras de datos más versátiles que Python (y otros lenguajes como R) ofrece para Machine Learning: el **DataFrame** que, básicamente, es una estructura tabular constituída por columnas que pueden contener datos de diversos tipos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pandas version \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **pandas** es otra librería que, si llegamos a manejar con soltura, nos ayudará a ser unos científicos de datos de lo más eficientes en Python. El DataFrame de `pandas` es la estructura tabular por excelencia que Python ofrece para el Machine Learning, además provee todo un arsenal de funciones que permiten procesarlo, rebanarlo (slicing), modificarlo y manipularlo de distintas formas.\n",
    "\n",
    "Hagamos una sencilla función que nos permite cargar los datos en un DataFrame de `pandas` (sí, sí...para dos líneas es tontería hacer un función, pero también es bueno acostumbrarnos a verlas...y a escribirlas :-) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data(housing_path, housing_filename):\n",
    "    \"\"\"\n",
    "    Esta función concatena el path del directorio donde se encuentra el fichero \n",
    "    total con el nombre de este formando así el path absoluto. Devuelve \n",
    "    el contenido del fichero como un DataFrame de pandas.\n",
    "    Argumentos:\n",
    "        - housing_path: el directorio donde el fichero total debe de estar alojado.\n",
    "        - housing_filename: el nombre del fichero total, con sus dos partes.\n",
    "    \"\"\"\n",
    "    csv_path = os.path.join(housing_path, housing_filename)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¡ALTO!** antes de cargar los datos en un DataFrame, duplicaremos manualmente algunas de las primeras líneas para ver su tratamiento posterior (abriremos **housing.csv** con algún editor de texto tipo [Notepad++](https://notepad-plus-plus.org/downloads/v7.8.4/) y duplicaremos alguna línea. No se debe utilizar Excel ya que puede hacer que el formato columnar se descuadre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si ya has \"tocado\" los datos a tu gusto...¡dale caña!\n",
    "housing = load_housing_data(HOUSING_LOCAL_PATH, HOUSING_CONCAT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con ayuda de la función `read_csv()`, por fín tenemos nuestros datos cargados en un DataFrame de `pandas`; la función `head()` de esta librería, permite obtener de  manera sencilla una visión de como es nuestro fichero (acordaos del \"lío\" que era hacer lo mismo directamente desde el fichero. Los DataFrame de `pandas` facilitan bastante la vida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igualmente la función `info()` nos muestra un resumen conciso de las características del DataFrame (número de columnas, nombre de las mismas, número de elementos no nulos, tipología de los mismos...). Vemos que disponemos de un dataset con, aproximadamente, 20640 elementos donde cada entrada es un distrito (cierto que el tamaño del dataset es reducido para un caso real, pero es adecuado para foguearse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisando duplicados\n",
    "\n",
    "Antes de profundizar mucho en nuestro dataset, es conveniente cerciorarnos de que no haya duplicados en el. Para ello tenemos varias instrucciones que nos pueden servir de ayuda: `duplicated()` por ejemplo marca como `True` todas aquellas filas (registros) que tienen el mismo valor en todos sus campos (marcandolas como `False` en caso contrario). Si quisiéramos solo centrarnos en algún campo en concreto le pasaríamos a `duplicated` una lista con los nombres de los campos a tomar en consideración:  `duplicated('campo1')` ó `duplicated(['campo1','campo2'...])`. **¡Ojo!** esta instrucción no elimina duplicados, ni modifica el DataFrame, solo los muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ojo al parámetro keep que determina qué registros son marcados como True (duplicados)\n",
    "# Si keep vale 'first' todos los registros duplicados salvo la primera ocurrencia son marcados como True.\n",
    "# Si keep vale 'last' todos los registros duplicados salvo la última ocurrencia son marcados como True.\n",
    "# Si keep vale False todas las ocurrencias de los registros duplicados son marcadas como True (ojo, False es un \n",
    "# tipo booleano, no va entrecomillado)\n",
    "housing[housing.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo siguiente sería eliminar los duplicados detectados, para ello utilizaríamos la sentencia `drop_duplicates()` indicando mediante el parámetro `keep` si queremos preservar la primera aparición (`'first'`, comportamiento por defecto), la última (`'last'`) ó eliminar todo duplicado (`False`). Además, el parámetro `inplace` permitirá que la eliminación de duplicados se haga in situ, sobre el propio DataFrame sin necesidad de asignar el resultado de la función a un nuevo DataFrame. Este parámetro es característico de todo método de `pandas` que conlleve posibles modificaciones del DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que al revisar de nuevo los primeros registros, hay huecos en el label del indexado...esto en si mismo no es un problema, pero hay que tener en cuenta que si se realizan manipulaciones sobre los datos que se apoyen en ese label, se pueden obtener resultados inesperados. Podemos resetear el label antes de que seguir realizando manipulaciones sobre los datos mediante `reset_index()`, que nos permitirá reiniciar el índice de manera que no haya huecos. Si esto del label y el index te suena algo raro, échale un vistazo a la diapositiva *00_project_Flow.ipynb (dataframes)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que los índices han recuperado su estado original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling time!!\n",
    "<img src=\"images/pickle.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginad que habéis empleado una tiempo importante en una determinada tarea que implica cargar una cantidad de datos en un objeto Python: desde scrapear datos de miles de webs, calcular millones de dígitos de pi o simplemente hacer cambios en un DataFrame pesado.\n",
    "\n",
    "Si la batería del portátil se termina de forma traicionera o simplemente la sesión de Python muere, esas estructuras de datos que tanto han costado generar se perderán (a veces la gente se pregunta \"pero cuando abro el notebook sigo viendo las trazas de ejecución...\", efectívamente, eso es todo lo que queda de tu ejecución anterior: las trazas :-) ).\n",
    "    \n",
    "`pickle` permite guardar un objeto Python (DataFrame de `pandas`, array de `numpy`, diccionario...) como un fichero binario en tu disco duro. Una vez guardado, ya da igual que tu sesión muera o que reinicies tu máquina. El objeto estará ahí disponible para que lo cargues cuando lo necesites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos a guardar el DataFrame en `pickle`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si existe otro pickle con el mismo nombre, se sobreescribirá\n",
    "housing.to_pickle('california.guau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borremos el DataFrame para hacer la demostración de cómo se vuelve a cargar\n",
    "del housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Restauramos la variable a partir del pickle\n",
    "housing = pd.read_pickle('california.guau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y si no estamos trabajando con `pandas`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probemos con un diccionario\n",
    "dictionary = {'x':[3,1,2,3], 'y':[9,2,1,3], 'status':[True, False], 'res':'A123'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos\n",
    "with open('dict.pickle', 'wb') as f:\n",
    "    pickle.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos el diccionario para asegurarnos de\n",
    "# que no exista ningún objeto con ese nombre cuando lo creemos\n",
    "del dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos\n",
    "with open('dict.pickle', 'rb') as f:\n",
    "    dictionary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intercambiando pickles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un objeto guardado mediante `pickle` puede ser compartido por correo, en un drive, en un USB...¡donde sea! Toda esta flexibilidad tiene el inconveniente de que cierta gente puede preparar pickles maliciosos que ejecuten algún tipo de código malicioso en tu máquina. Así que es conveniente solo cargar aquellos pickles cuya fuente se conozca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haciendo nuestro dataset algo más interesante\n",
    "\n",
    "Nuestro dataset es bastante aburrido, **no tiene ni un valor nulo ni tampoco features categóricas**...vamos a intentar retocar algunos de los datos para darle un poquito de emoción a esto.\n",
    "Vamos a crearnos una función que aplique una máscara a una de nuestras features de manera que un determinado % de sus valores se vuelva nulo preservando el resto. Para más información sobre el método `mask`, echa un vistazo a la diapositiva *00_project_Flow.ipynb (mask)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numpy version \" + np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **numpy** librería fundamental para cualquier científico de datos que opte por desarrollar su actividad en Python; `numpy` es *LA LIBRERÍA* para manipular vectores y estructuras matriciales de cualquier dimensión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_null_by_column(column, pandas_dataframe, null_pctge=.15):\n",
    "    \"\"\"\n",
    "    Esta función tiene por objeto establecer a NaN un determinado\n",
    "    porcentaje de los valores de la columna que se indique.\n",
    "    Argumentos:\n",
    "        - column: el nombre de la columna que se quiere nulificar.\n",
    "        - pandas_dataframe: el dataframe de pandas que queremos modificar.\n",
    "        - null_pctge: porcentaje de valores de la columna que deben de ser modificados.\n",
    "    \"\"\"\n",
    "    # shape es una propiedad de todos objeto DataFrame de pandas\n",
    "    col_shp = pandas_dataframe[column].shape\n",
    "    # Creamos una máscara con dos posibles valores True/False\n",
    "    mascara_nan = np.random.choice([True, False], size=col_shp, p=[null_pctge,1-null_pctge])    \n",
    "    # Al aplicar la máscara al DataFrame, ojo, True reemplazará el valor del campo elegido por NaN y False lo preservará\n",
    "    pandas_dataframe[column] = pandas_dataframe[column].mask(mascara_nan)\n",
    "    return pandas_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elegiremos por ejemplo la feature totalBedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = set_random_null_by_column(\"totalBedrooms\", housing, null_pctge=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno...vemos que uno de nuestros campos ya tiene unos cuantos valores NaN lo cual hará que nos tengamos que encargar de hacer algo con los datos antes de alimentar un modelo con ellos. Pero no es suficiente destrozo; vivimos en un mundo de categorías y como tal **no es concebible un dataset sin variables categóricas**, así que vamos a insertar una que asigne la proximidad al oceano de cada distrito. Empezaremos definiendo los valores de dicha categoría y la probabilidad de que aparezcan en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_proximity_cat = ['<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'NEAR BAY' ,'ISLAND']\n",
    "# Esto de abajo son probabilidades...sí, parecen raras pero suman 1\n",
    "ocean_proximity_prob = [.44, .32, .12, .11 , .01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos un array de `numpy` en el que se distribuyan las categorías según las probabilidades indicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_prox_values = np.random.choice(ocean_proximity_cat, size=(housing.shape[0],1), p=ocean_proximity_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ocean_prox_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La creación de una nueva columna en nuestro DataFrame es tan fácil como referenciarla como si ya existiera y asignarle sus valores mediante un array de `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"] = ocean_prox_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ya tenemos nuestra nueva columna categórica; podemos comprobar cómo se han distribuído las categorías entre los distritos mediante la función `value_counts()`. Normalmente aquellos valores que se repiten con frecuencia en un dataset, tienen altas probabilidades de corresponder a una feature categórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisando nuestro dataset\n",
    "\n",
    "Vamos a ver dos maneras de abordar la exploración de nuestro dataset, primero una más numérica y luego otra más gráfica. Para la primera de ellas, la función `describe()` permite obtener un resumen estadístico de la distribución de nuestras features numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Aquí tenemos información que puede ser interesante (ojo a cómo afecta la presencia de NaN a cada estadístico),\n",
    "* **count** Contabiliza todos aquellos valores que no son NaN.\n",
    "* **mean** Calcula la media de los valores para cada feature (de nuevo, se ignoran los valores NaN).\n",
    "* **min** Obtiene el mínimo valor para cada feature.\n",
    "* **max** Obtiene el máximo valor para cada feature.\n",
    "* **std** Muestra la desviación estándar proporcionando una medida de la dispersión de los valores con respecto a la media (mean), cuanto mayor sea la desviación estándar, menos representativa será la media de la distribución.\n",
    "* **25%**,**50%**,**75%** Los cuartiles representan el valor por debajo del cual se situan un determinado porcentaje de los valores para una determinada feature. Por ejemplo el primer cuartil de la feature population nos indica que en el 25% de los distritos viven menos de 787 personas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matplotlib version \" + matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Algo raro en el medianIncome, housingMedianAge y medianHouseValue?¿Algo que resaltar sobre las colas de las distribuciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. La feature **medianIncome** que, en principio, parecería lógico que estuviera expresada en USD, no parece estarlo. Consultando con el equipo encargado de recolectar los datos, nos dicen que han hecho un escalado de los datos de manera que el valor máximo es ahora 15 y el valor mínimo 0.5 Es normal encontrarse con escalados de este tipo y no es algo necesariamente malo, pero sí que es conveniente conocer la lógica que se ha seguido.\n",
    "\n",
    "2. Las features **housingMedianAge** y **medianHouseValue** tienen un pico muy sospechoso en sus valores más alto. El problema es que la segunda de ellas es nuestra label o variable target y si los datos se han dispuesto de tal manera que todos aquellos valores de medianHouseValue mayores de 500000 se reflejarán como 500000, nuestro modelo puede aprender que es el tope con el que puede valorar una propiedad. Aquí tenemos dos opciones,\n",
    "    * Que para los distritos afectados por esto, se recolecten los valores verdaderos.\n",
    "    * Eliminar todos los distritos de nuestros datos cuyo medianHouseValue ha sido limitado (tanto del training como del test set) *housing = housing[housing.medianHouseValue < 500000]*\n",
    "    \n",
    "3. Parece que predominantemente tenemos distribuciones asimétricas hacia la derecha, esto es, la media caerá a la derecha de la mediana. Esto puede hacer que nuestros modelos tengan más problemas a la hora de detectar patrones que si la distribución fuera más simétrica.\n",
    "\n",
    "<img src=\"images/asymmetry.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muerte por corchetes\n",
    "¿Existe alguna manera un poco más simple y legible de indexar un DataFrame que no sea apelotonando corchetes y paréntesis? Sí, se puede transformar esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[((housing['medianHouseValue'] > 500000) & (housing['totalBedrooms'].notnull())) | (housing['ocean_proximity'] == 'ISLAND')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.query('(medianHouseValue > 500000 and not totalBedrooms.isnull()) or (ocean_proximity == \"ISLAND\")', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aquellas personas de bien que les sangren los ojos con el hardcoding, existe también la posibilidad de referenciar variables de nuestro entorno en la query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuestra variable\n",
    "carisimo = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La incorporamos a la query precediéndola de @\n",
    "housing.query('(medianHouseValue > @carisimo and not totalBedrooms.isnull()) | (ocean_proximity == \"ISLAND\")',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si alguna persona no se siente confortable con el número de filas mostrado por pantalla\n",
    "# se puede modificar mediante la siguiente opción\n",
    "pd.set_option('display.max_rows',10)\n",
    "# En este caso el número de columnas no es un problema...pero si lo fuera, la siguiente opción\n",
    "# permite ajustar dicho número\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo mejor de todo, es que independientemente de la legibilidad, `query()` nos permite además optimizar el uso de la memoria. Cualquier expresión compuesta, conlleva por debajo la creación de arrays temporales auxiliares. Esto en principio no es un problema a no se que el tamaño de nuestro array o nuestro DataFrame (normalmente estos son más propensos a crecer desmesuradamente que los arrays) se incremente en exceso, pero es conveniente tenerlo en cuenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro`engine`, puede plantear incógnitas ya que no es algo que esté ampliamente documentado. Para el método `query()`, engine puede tomar dos valores:\n",
    " * `numexpr`: el valor por defecto, en la mayoría de las casuísticas es la elección más adecuada en términos de rendimiento.\n",
    " * `python`: la opción que utilizaremos si queremos meter en nuestras \"queries\" funciones de Python, por lo general su rendimiento es inferior a `numexpr`, tan solo en el tramo de datasets que rondan los 15K-20K registros y tres o cuatro features, llega a mejorar a `numexpr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación datos en conjuntos de train y test\n",
    "\n",
    "En este punto nos despediremos de una parte de nuestros datos hasta el final del proceso: nunca los utilizaremos, los miraremos o pensaremos en ellos hasta que toque evaluar el error de generalización del modelo elegido.\n",
    "\n",
    "Si tomamos en consideración los datos de test a la hora de escoger nuestro modelo, cuando llegue el momento de obtener el error de generalización nos encontraremos con que el desempeño del modelo es espectacular PERO al ponerlo en Producción nos daremos de bruces con la cruda realidad y es que, muy probablemente, nuestro modelo no tendrá un performance tan bueno como creíamos (sobreajuste u overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Data Snooping Bias** una de las consecuencias posibles de no respetar los datos de test y utilizarlos también para detectar patrones sobre ellos; es la posibilidad de que algunos de los patrones detectados sean puramente aleatorios y que no se cumplan en los nuevos datos a los que nuestro modelo tenga que enfrentarse de modo que este tendrá un pobre desempeño ante nuevos datos. Una manera de evitar esto es, justamente, el uso de un conjunto de test completamente desconocido para nuestro modelo sobre el que probará si realmente es un modelo balanceado y capaz de generalizar o peca de sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`scikit-learn` proporciona varios métodos para separar datasets en múltiples subconjuntos de distintas maneras (por ejemplo, `train_test_split`). Estos métodos, tienen como particularidad que pueden recibir como parámetro una semilla de manera que, aunque los ejecutemos múltiples veces, la separación resultante será la misma que la primera vez; esto es algo muy interesante ya que de no ser así, estaríamos incurriendo en el **data snooping bias**. \n",
    "Además puede recibir múltiples datasets siempre y cuando tengan idéntico número de filas, la utilidad de esto es que si por ejemplo nos pasan las labels en otro DataFrame diferente a las features, garantizamos que la división entre train set y test set será idéntica en ambos DataFrame.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero...**¿basta con introducir aleatoriedad en el particionamiento?** es posible que si el dataset es muy grande sí; pero existe el riesgo de introducir **sampling bias**. Si tenemos una población que está dividida en grupos homogéneos o estratos, es necesario que en nuestros subconjuntos de training y test, se mantenga la proporción entre dichos estratos. Para más información sobre esto, echa un vistazo a la diapositiva *00_project_Flow.ipynb (sampling bias)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/digest.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que hemos hablado con los expertos que se encargan actualmente de realizar los cálculos y nos han dicho que para predecir el precio medio de la propiedad, **es muy importante la feature medianIncome**, es decir la renta promedio; por tanto nos interesa que los distintos \"subgrupos\" de medianIncome estén proporcionalmente representados en nuestros conjuntos de training y test...pero hay otro problema ¿cómo obtenemos los subgrupos de una feature con valores numéricos continuos?. Vamos a ello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Empezamos visualizando medianIncome\n",
    "housing.hist(column=\"medianIncome\", bins=50, figsize=(4,4))\n",
    "plt.title(\"medianIncome\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La integración de `matplotlib` con `pandas`, nos permite obtener visualizaciones rápidas y eficaces. Aunque puede ser que a veces busquemos algo más interactivo. Una manera relativamente asequible de conseguir esta interactividad es `hvplot`. [Aquí](https://hvplot.holoviz.org/user_guide/Pandas_API.html) tenéis una referencia a la API de `pandas`. **Recordad que para que os funcione el siguiente import, previamente tendréis que haber instalado la librería del modo que se indicaba en el LEEDME.docs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.hvplot.hist(y=\"medianIncome\", bins=50, width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinando el histograma para medianIncome, vemos que la mayoría de los valores de dicha feature, se agrupan en torno a 1.5 y 6, aunque hay algunos que se alejan bastante de 6. Los estratos que conformemos tienen que estar lo suficientemente bien representados. Esto se traduce en no volvernos locos definiendo demasiados estratos y que, por tanto, cada estrato sea lo suficientemente grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vamos a discretizar los valores de medianIncome.\n",
    "# Optaremos por definir cinco estratos que etiquetaremos como\n",
    "# 1, 2, 3, 4 y 5. \n",
    "# El primer estrato irá de 0 a 1.5\n",
    "# El segundo de 1.5 a 3\n",
    "# El tercero de 3 a 4.5\n",
    "# El cuatro de 4.5 a 6\n",
    "# El quinto y último de 6 en adelante\n",
    "# Realmente esto no es algo escrito en piedra, podéis jugar con estos rangos\n",
    "# y comprobar si los estratos obtenidos son adecuados de acuerdo a los consejos \n",
    "# que hemos visto.\n",
    "housing['incomeCat'] = pd.cut(housing['medianIncome'], bins=[0, 1.5, 3, 4.5, 6, np.inf], labels=[1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hemos creado una nueva feature llamada incomeCat que \n",
    "# es una versión discretizada de medianIncome.\n",
    "# Podéis echar un ojo a los 10 primeros registros para ver\n",
    "# que los valores de medianIncome han ido a parar al estrato esperado\n",
    "housing[['medianIncome', 'incomeCat']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo tan simple como pudiera parecer un histograma de una feature categórica, no es trivial de conseguir con `matplotlib`. Lo bueno que existen otras opciones para lograrlo sin demasiado esfuerzo. Como por ejemplo `seaborn` que pone una capa de abstracción sobre `matplotlib` lo cual consigue que la interacción con esta sea algo más amigable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Seaborn version \" + sns.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(housing['incomeCat'], color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se hará el muestreo estratificado en base a la categoría sintética que hemos creado, lo haremos utilizando el parámetro `stratify` de `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set, strat_test_set = train_test_split(housing, test_size=0.2, random_state=42, stratify=housing['incomeCat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cual era la proporción inicial de las categorías ó estratos resultantes de incomeCat para el dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"incomeCat\"].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora comprobemos que las proporciones se corresponden con los conjuntos de training y test resultantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Conjunto ó dataset de entrenamiento (training)\n",
    "strat_train_set[\"incomeCat\"].value_counts() / len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Conjunto ó dataset de prueba (test)\n",
    "strat_test_set[\"incomeCat\"].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos el stratified sampling nos ha servido para mantener en nuestros conjuntos de train y test la misma proporción de categorías de incomeCat que en el conjunto inicial. Ahora ya podemos prescindir de esta feature auxiliar que, no olvidemos, habíamos creado a partir de medianIncome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data_set in [strat_train_set, strat_test_set, housing]:\n",
    "    data_set.drop([\"incomeCat\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible que hayáis obtenido un maravilloso warning del paso anterior. Un warning es lo suficientemente importante para que investiguemos su causa.\n",
    "El caso de SettingWithCopyWarning, tiene por objeto poner de manifiesto, aquellas operaciones con DataFrames para cuyo resultado no siempre está claro si lo que se obtiene es una vista del DataFrame (*view*) o una copia (*copy*). Esto es grave porque:\n",
    "* Modificar una vista de una DataFrame implicaría modificar el DataFrame original.\n",
    "* Modificar una copia de un DataFrame, no implica modificar el original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este warning parece decirnos que no está claro si `train_test_split` devuelve copias de nuestro DataFrame original o vistas. Navegando un poco nos damos cuenta de que en este caso es debido a una especie de descoordinación entre `scikit-learn` y `pandas`:\n",
    "https://github.com/scikit-learn/scikit-learn/issues/8723\n",
    "\n",
    "Dicho lo cual, en este caso el warning no tiene mayor influencia en nuestro código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha costado pero de momento hemos dejado fino el tema del particionamiento de datos en training y set. Podemos avanzar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de datos\n",
    "\n",
    "Hasta el momento la toma de contacto con los datos no ha sido demasiado profunda, aunque ya hemos hecho algunas cosas. Para los siguientes pasos nos aseguraremos de,\n",
    "1. El conjunto de test está apartado y aislado, nos olvidaremos de el por completo.\n",
    "2. La exploración de datos se realizará solo en base al training set y si este es muy grande, obtendremos de el un set de exploración para que esta sea rápida y eficiente.\n",
    "\n",
    "En nuestro caso el conjunto de training es manejable, así que directamente nos haremos una copia del mismo para no cargárnoslo en alguna manipulación. ¡Ojo! el método `copy()` es bastante importante ya que en principio podemos pensar que con una simple asignación a través de `=`, sería suficiente para crearnos un nuevo DataFrame...si hacemos esto lo que realmente estaremos creando es una referencia al DataFrame original, de manera que si modificamos la referencia, el original también se verá afectado (y viceversa). Con el método `copy()`, manteniendo el valor de su único parámetro `deep` a `True` (por defecto es así), se consigue un copia completa de manera que tengamos dos DataFrame independientes. ¿Copias y referencias? si te suena raro, échale un vistazo a la diapositiva *00_project_Flow.ipynb (Sabiendo asignar)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing será un DataFrame obtenido mediante el método copy()\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing2 será una referencia al DataFrame original\n",
    "housing2 = strat_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cojamos un label al azar de nuestro DataFrame, por ejemplo: 17606\n",
    "# Veamos el valor de strat_train_set para una determinada feature\n",
    "# y el índice escogido\n",
    "strat_train_set.loc[17606, 'medianIncome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y también para la referencia...\n",
    "housing2.loc[17606, 'medianIncome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cambiemos el valor en la referencia\n",
    "housing2.loc[17606, 'medianIncome'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Por ser una referencia...afecta también al DataFrame original\n",
    "strat_train_set.loc[17606, 'medianIncome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizando de forma fácil e informativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que tenemos entre las features latitud y longitud (latitude y longitude en el dataset) vamos a ver que ocurre si disponemos los registros que tenemos en base a sus coordenadas (al final es una excusa para introducir otro método de `pandas`: el método `plot()`, que invoca por debajo a `matplotlib` y que permite crear sobre nuestro DataFrame, distintos tipos de visualizaciones: líneas, barras, tartas, histogramas...). Probemos con un gráfico de dispersión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bueno...no nos han engañado, se parece bastante a California...pero tampoco es que nos aporte mucho. El método `plot` es muy potente pero a cambio, para sacarle provecho, requiere de nuestra parte no solo que consultemos la documentación relativa al método, sino que además tengamos en cuenta aquellos parámetros de `matplotlib` propios de la visualización que queremos plasmar.\n",
    "Por ejemplo, en el caso del gráfico de dispersión (scatter plot) existe un parámetro llamado **alpha** que permite destacar aquellas zonas con mayor densidad de puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Aquí ya tenemos algo más de información! Vemos una mayor densidad en las zonas correspondientes con las grandes áreas urbanas de California: Los Angeles, San Francisco, San Diego, Fresno, Sacramento...Pero aun podemos meter más contenido en el mapa, para ello utilizaremos dos propiedades del tipo de plot que estamos visualizando (un **scatterplot**): el color y el tamaño (size).\n",
    "\n",
    "- **c** representa el color. Permite especificar la feature cuyo valor determinará el color del punto y que se traducirá en un color determinado por:\n",
    "- **cmap** : el [mapa de colores](https://matplotlib.org/stable/tutorials/colors/colormaps.html) seleccionado.\n",
    "- **s** el tamaño de cada punto vendrá determinado por el valor de la feature que asignemos a este parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]/100, label=\"population\",\n",
    "            c=\"medianHouseValue\", cmap=plt.get_cmap(\"autumn_r\"), colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí ya se puede ver más claramente que, en apariencia, los precios más altos coinciden con grandes núcleos urbanos cercanos a la costa. De nuevo, hay que tener en cuenta que el obtener una visualización adecuada no es algo trivial, sino que requiere de probaturas y de darle varias vueltas: por ejemplo algo tan simple como dividir population entre 100 para evitar que el tamaño de los puntos se nos vaya de las manos, ó encontrar una paleta de color adecuada para cmap..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la versión con `hvplot`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models.formatters import BasicTickFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.hvplot.scatter(x=\"longitude\", y=\"latitude\", \n",
    "                       alpha=0.4, s=housing[\"population\"]/100, \n",
    "                       label=\"population\", c=\"medianHouseValue\", \n",
    "                       cmap=plt.get_cmap(\"autumn_r\"), colorbar=True).opts(plot=dict(colorbar_opts={'formatter': BasicTickFormatter(use_scientific=False)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Outliers\n",
    "\n",
    "<img src=\"images/outlier.png\">\n",
    "\n",
    "#### Definición\n",
    "Se entiende por outlier, aquel valor que es distante numéricamente del grueso de la distribución a la que pertenece.\n",
    "El origen de un outlier puede estar en errores humanos o mecánicos en su captura o puede que sean valores auténticos, pero extremos en el dataset.\n",
    "Incluso su presencia puede ser indicativo de algo. Por ejemplo en un dataset de transacciones, puede ayudar a identificar movimientos fraudulentos.\n",
    "\n",
    "La definición de outlier es tan sencilla como vaga. Y es que en última instancia lo que es un outlier y lo que no, es un ejercicio algo subjetivo y que depende en buena parte del caso de negocio al que refiera el problema.\n",
    "\n",
    "* ¿Cómo de outlier es un outlier? No siempre es ideal quitar outliers que *no lo son mucho*.\n",
    "* ¿Qué ocurre si nuestro dataset tiene un gran número de features? ¿Para cuántas de ellas una observación tiene que contener outliers de manera que decidamos prescindir de ella?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la práctica en problemas con muchas features es poco común atreverse a quitar outliers por algún lado. Hay modelos además que son más sensibles a outliers (como por ejemplo la regresión logística y los SVM) y otros que los suelen asimilar sin mucho problema (como los árboles de decisión o K-nearest neighbours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detección\n",
    "Algunos métodos para la detección de outliers:\n",
    "##### Análisis de valores extremos (Extreme Value Analysis)\n",
    "El objetivo de este método es identificar las colas de las distribuciones de las features y encontrar los valores ubicados en sus extremos. En una distribución gaussiana, los outliers cumplirán alguna de estas características:\n",
    "* Mayores que la media + 3 veces la desviación típica.\n",
    "* Menores que la media - 3 veces la desviación típica.\n",
    "\n",
    "Segúramente suene familiar la [**regla 68–95–99.7**](https://en.wikipedia.org/wiki/68–95–99.7_rule). Básicamente esta regla nos dice que para una distribución normal el 68% de los valores se encuentran a una desviación típica de la media, el 95% a dos desviaciones típicas y el 99.7% a tres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente apartado (Z-score), veremos un método rápido de obtener los outliers en base a este criterio, aunque siempre es posible calcularlo de manera \"manual\" obteniendo la media y la desviación típica para una feature mediante los métodos `mean()` y `std()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centrémonos ahora en el caso más habitual...*¿qué pasa si nuestra distribución NO es normal?*.\n",
    "En este caso se puede recurrir al **rango intercuartílico**:\n",
    "<img src=\"images/IQR.png\">\n",
    "* Primero se calcula dicho rango: IQR = cuantil 75 (tercer cuartil) - cuantil 25 (primer cuartil)\n",
    "* Límite superior = cuantil 75 (tercer cuartil) + (IQR * 1.5)\n",
    "* Límite inferior = cuantil 25 (primer cuartil) - (IQR * 1.5)\n",
    "Es posible que el 1.5 haya que reemplazarlo por 3 para casos más extremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tomamos de nuevo como ejemplo medianIncome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el rango intercuartílico\n",
    "IQR = housing[\"medianIncome\"].quantile(0.75) - housing[\"medianIncome\"].quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el límite superior normal y extremo (en el caso del inferior la operativa\n",
    "# sería muy parecida)\n",
    "limSup = housing[\"medianIncome\"].quantile(0.75) + (IQR *  1.5)\n",
    "limSupExt = housing[\"medianIncome\"].quantile(0.75) + (IQR *  3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver la proporción de observaciones que se encuentra por encima de estos límites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de observaciones\n",
    "total = housing['medianIncome'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examinamos el límite superior calculando la proporción\n",
    "len(housing[housing['medianIncome'] > limSup])/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto quiere decir que un 3% de las observaciones están por encima del límite superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examinamos el límite superior extremo\n",
    "housing[housing['medianIncome'] > limSupExt].shape[0]/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tan solo un 0.6% de las observaciones están por encima del límite superior extremo. Se procedería de forma análoga para revisar aquellos valores situados a la izquierda de la distribución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Z-Score\n",
    "Es una medida de la distancia, en términos de desviaciones estándar, a la que una observación se encuentra de la media. Esta medida puede ser positiva (indicando que la observación se encuentra por encima de la media) o negativa (indicando que la observación se encuentra por debajo de la media).\n",
    "\n",
    "Una de las ventajas del Z-score es que permite eliminar los efectos de la escala de los datos (es decir, da igual que los datos se muevan entre 0 y 13 que entre -2823020.2 y 49330202222.3) esto permite comparar features y datasets diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = list(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los z-score para cada feature\n",
    "for feat in header:\n",
    "    try:\n",
    "        z = stats.zscore(housing[feat])\n",
    "        print(\"feature: \" + feat + \". Max z-score: \" + str(z.max()) + \". Min z-score: \" + str(z.min()))\n",
    "    except TypeError:\n",
    "        print(\"Can't calculate z-score for feature \" + feat + \". It's \"+ str(type(housing[feat].values[0])))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tendríamos unos cuantos extremos que investigar, por poner un ejemplo\n",
    "# vamos a echar un vistazo a la distribución de la feature population que se va \n",
    "# más allá de 30...\n",
    "z = stats.zscore(housing['population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Examinemos pues aquellos z-score mayores que 30\n",
    "print(np.where(z > 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ¿A qué registros corresponden esos z-score? lo sabremos a partir de los índices devueltos por numpy where\n",
    "housing.iloc[np.where(z > 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un gráfico de dispersión a veces nos da perspectiva de nuestros posibles outliers\n",
    "plt.scatter(np.arange(len(housing)), housing['population'], c=z>10, cmap=plt.get_cmap(\"winter\"), alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a rizar un poco más el rizo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering\n",
    "<img src=\"images/kmeans.gif\">\n",
    "\n",
    "Cortesía de [Jeremy Jordan](https://www.jeremyjordan.me/grouping-data-points-with-k-means-clustering/)\n",
    "\n",
    "El objetivo de las técnicas de Clustering, es agrupar en clusters o conjuntos, aquellas observaciones más similares entre sí (el concepto de similitud puede vernir dado por una métrica del tipo distancia Euclídea por ejemplo). Los pasos básicos de un algoritmo típico de clustering como el K-means son:\n",
    "* Determinar el número de clusters que se quiere obtener (`k`).\n",
    "* Asignar las `k` primeras observaciones como centroides provisionales de los `k` clusters.\n",
    "* Calcular la distancia escogida (por ejemplo la Euclídea) de cada observación con respecto a cada centroide y en función del resultado asignar cada observación al cluster correspondiente al centroide más próximo.\n",
    "* Después de haber completado los pasos anteriores, recalcular cada centroide en base a la media de todas las observaciones asignadas a su cluster.\n",
    "* Repetir el proceso de cálculo de distancias, asignación de observaciones y recálculo de centroides hasta que no haya cambios en las asignaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import kmeans, vq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroides, inercia...\n",
    "centroids, avg_distance = kmeans(housing['medianIncome'], 5)\n",
    "groups, cdist = vq(housing['medianIncome'], centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Para elegir k de manera algo más informada, ilustramos \n",
    "# el ejemplo con medianIncome, cuyos estratos ya hemos analizado\n",
    "# anteriormente.\n",
    "y = np.arange(0,housing['medianIncome'].shape[0])\n",
    "plt.scatter(housing['medianIncome'],  y , c=groups)\n",
    "plt.xlabel('Salaries')\n",
    "plt.ylabel('Indices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de determinar el número óptimo de clusters existen diversas estrategias, una de las más populares (ilustrada abajo) es la regla del codo (elbow rule). Otro método ampliamente utilizado es el Silhouette analysis que basicamente asigna una puntuación (entre -1 y 1) a cada observación dependiendo de:\n",
    "* La distancia de dicha observación al centro de su propio cluster (`a`).\n",
    "* La distancia de dicha observación al cluster más cercano (`b`).\n",
    "\n",
    "Silhouette coefficient para una muestra = `(b - a) / max(a, b)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a echar un vistazo a la regla del codo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el número de clusters con el que queremos probar\n",
    "nClusters = np.arange(1,13)\n",
    "distances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos kmeans para cada una de las posibles configuraciones\n",
    "for nc in nClusters:\n",
    "    centroids, avg_distance = kmeans(housing['medianIncome'], nc)\n",
    "    distances += [avg_distance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez tenemos la distancia media a los centroides para\n",
    "# cada una de las configuraciones, pintamos el conjunto.\n",
    "# La clave está donde se forma el codo.\n",
    "plt.plot(nClusters, distances)\n",
    "plt.xticks(nClusters)\n",
    "plt.title(\"Elbow rule\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Avg. distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nuevo: no es algo trivial la detección de outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aproximación gráfica\n",
    "\n",
    "Se trata de recurrir a distintas visualizaciones para comprobar si pueden existir o no outliers evidentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.hist(housing[\"medianIncome\"], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogramas, gráficos de dispersión, de bigotes...con `matplotlib` y `seaborn`, se pueden abarcar todas estas posibilidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocesamiento de outliers\n",
    "Algunas posibilidades para preprocesar outliers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputación de valores missing al uso\n",
    "Si se tiene la certeza de que los outliers detectados, se deben a algún error durante el proceso de recogida, a efectos prácticos se pueden considerar valores *missing* y se procederá a su imputación como si de tales se trataran (imputando la media, la mediana, la moda...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trimming\n",
    "Eliminar aquellos outliers directamente del dataset. Logicamente este método no es viable si el número de outliers es alto. La técnica para realizar esto es la que se utilizó anteriormente cuando se planteó el problema del techo en los valores del label medianHouseValue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturamos aquellas observaciones cuyo medianHouseValue sea superior a 500000\n",
    "index = housing[housing['medianHouseValue'] >= 500000].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SANITY CHECK: Comprobamos que al hacer el drop de dichas observaciones se consigue el objetivo deseado\n",
    "assert (housing.drop(index)[\"medianHouseValue\"] >= 500000).value_counts().get(True, None) is None, \\\n",
    "\"Sigue habiendo distritos con medianHouseValue mayor o igual que 50000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo bien, ahora ya se puede ejecutar\n",
    "housing.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top/Bottom/Zeroing\n",
    " * Si se sabe que para una determinada feature existe un valor que no puede ser excedido. Lo que se puede hacer es: primero identificar las observaciones que exceden ese valor y luego asignarlas ese tope que no puede ser excedido.\n",
    " * Análogamente, *Bottom* se aplicará al lado izquierdo de la distribución: aquellos valores por debajo de un determinado umbral serán configurados a ese umbral.\n",
    " * *zeroing* es un caso similar a los anteriores que se aplica para el caso de features que no pueden tomar valores negativos. De este modo serán considerados outliers aquellos valores que sí que tomen valores negativos y se procesarán asignándoles cero como valor.\n",
    " \n",
    "Veamos como abordar el primer caso (el resto serían muy similares):\n",
    " * Por ejemplo se desea que la feature medianIncome no sea mayor de 13 en ningún caso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por curiosidad...¿cuántos distritos cumplen con la condición?\n",
    "wealthyBlocks = len(housing[housing[\"medianIncome\"] > 13])\n",
    "\n",
    "# Seguido se cacula la proporción\n",
    "wealthyBlocks/len(housing[\"medianIncome\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Muy poquitos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se sustituye por el valor deseado\n",
    "housing.loc[housing[\"medianIncome\"] > 13, \"medianIncome\"] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se comprueba que ha funcionado, ahora 13 es el tope de medianIncome\n",
    "housing[\"medianIncome\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discretización\n",
    "\n",
    "Se trata de un proceso en el cual una feature continua se transforma en una feature discreta de forma que sus valores se reparten en un conjunto de intervalos que cubre todos los valores de dicha feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlaciones\n",
    "\n",
    "Puesto que nuestro dataset no es muy grande, se puede calcular de forma poco costosa el coeficiente de correlación de Pearson (Rxy) que nos permite descubrir relaciones **lineales** de dependencia entre variables independientemente de la escala en la que se midan estas.\n",
    "* Si Rxy = 1, existe una correlación positiva perfecta. Existe una dependencia total entre las dos variables (denominada relación directa) de modo que si una de ellas aumenta, la otra también lo hace en una proporción constante.\n",
    "* Si 0 < Rxy < 1, existe una correlación positiva.\n",
    "* Si Rxy = 0, no existe relación lineal, aunque esto no quiere decir que ambas variables sean independientes, pueden existir aun relaciones no lineales entre ellas.\n",
    "* Si Rxy -1 <  Rxy < 0, existe una correlación negativa.\n",
    "* Si Rxy = -1, existe una correlación negativa perfecta. Existe una dependencia total entre la dos variables (denominada relación inversa) de modo que si una de ellas aumenta, la otra disminuye en proporción\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "En la explicación anterior se ha señalado en negrita la palabra **lineales** y es que el coeficiente de correlación se centra en este tipo de relaciones que consisten por ejemplo en,\n",
    "* Si el valor una variable X aumenta, el valor de otra variable Y lo hace también o por contra disminuye. \n",
    "\n",
    "Pero existen otro tipo de relaciones **no lineales** que este coeficiente no recoge, \n",
    "* Supongamos por ejemplo que si el valor de la variable X es próximo a 0.5, el valor de Y disminuye.\n",
    "\n",
    "A continuación algunos ejemplos de correlaciones (imagen obtenida de [Wikipedia](https://en.wikipedia.org/wiki/Correlation_and_dependence#/media/File:Correlation_examples2.svg))\n",
    "* En la última fila apreciamos que, aunque existe algún tipo de independencia, esta no puede ser explicada mediante relaciones lineales.\n",
    "* En la segunda línea la conclusión que podemos sacar es que la fuerza la correlación no tiene que ver necesariamente con lo pronunciado de la pendiente en la gráfica resultante\n",
    "<img src=\"images/correlation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restauremos el DataFrame para resturar potenciales outliers\n",
    "# que hayamos quitado y así poder seguir explorando posibles anomalías\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la instrucción que acabamos de ejecutar, ya tendríamos las correlaciones. Si queremos ponerlas bonitas, tendremos que dedicarle tiempo y probar las distintas opciones que nos ofrece `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Indicamos las dimensiones de la figura\n",
    "fig = plt.figure(1, figsize=(6,6))\n",
    "# Puesto que solo queremos que tenga un subplot lo indicamos mediante 111\n",
    "ax = fig.add_subplot(111)\n",
    "labels = ['labels']+corr_matrix.columns.tolist()\n",
    "ax.set_xticklabels(labels, rotation='45', ha='left')\n",
    "ax.set_yticklabels(labels, rotation='horizontal', ha='right')\n",
    "\n",
    "corr_mat_plot = ax.matshow(corr_matrix, cmap=plt.cm.hot_r)\n",
    "# Con esto indicamos explicitamente que el rango de nuestros valores será -1,1\n",
    "corr_mat_plot.set_clim([-1,1])\n",
    "cb = fig.colorbar(corr_mat_plot)\n",
    "cb.set_label(\"Correlation Coefficient\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque a veces [\"seaborn\"](https://seaborn.pydata.org/introduction.html) nos hace la vida un poco más fácil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(corr_matrix, \n",
    "            annot=True,\n",
    "            cbar_kws={\"label\": \"Correlation coefficient\", \"shrink\": 1.25, \"ticks\": np.linspace(-1.0, 1.0, 9)}, \n",
    "            cmap=plt.cm.hot_r, \n",
    "            vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y si queréis algo más interactivo, `hvplot` puede ayudar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_matrix.hvplot.heatmap(flip_yaxis=True, rot=45, cmap=plt.cm.hot_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviando la diagonal de la matriz (que relaciona cada feature consigo misma) tenemos,\n",
    "* Una muy fuerte **relación directa** entre **totalRooms** y **totalBedRooms**, lo cual es razonable (cuanto más habitaciones haya en un distrito más probable es que haya más dormitorios)\n",
    "\n",
    "* También se ve una muy fuerte **relación directa** entre **population**, **totalRooms** y **households** (lo cual también parece razonable)\n",
    "\n",
    "* Vemos algo que parece más interesante que lo anterior y es que parece que existe importante **relación directa** entre la renta media (**medianIncome**) y nuestra variable target **medianHouseValue**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"medianHouseValue\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra de las funcionalidades que `pandas` ofrece apoyándose en `matplotlib` es `scatter_matrix()`, que nos dibuja una matriz en la que cada elemento es un gráfico de dispersión entre cada feature que reciba como parámetro, solo hay que imaginarse una matriz similar a la anterior reemplazando cada pequeña celda coloreada por un gráfico de dispersión, sería grande ¿verdad? Por eso solo vamos a pintarlo para las tres variables que parecen algo más correladas con medianHouseValue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"medianHouseValue\", \"medianIncome\", \"totalRooms\", \"housingMedianAge\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que, como la matriz de correlaciones ya apuntaba, la relación aparentemente más prometedora es entre medianIncome y medianHouseValue. Todo un detalle por parte de `pandas` es que en la diagonal principal, en vez de mostrarnos el gráfico de dispersión de una variable contra si misma, nos muestra el histograma de esa variable (bastante más informativo).\n",
    "Vamos a hacer zoom sobre esta posible relación,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"medianIncome\", y=\"medianHouseValue\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante este zoom ya que se aprecian tres líneas algo sospechosas,\n",
    "* La que ya conocíamos que situaba un techo de 500000 USD para medianHouseValue\n",
    "* Una segunda que parece asomarse alrededor de los 450000 USD\n",
    "* Una tercera que se aprecia claramente sobre los 350000 USD\n",
    "\n",
    "Esto es algo de lo que se podría informar al equipo de recolección de datos, por si tuvieran alguna explicación; podría ser recomendable intentar quitar esos distritos de nuestro set para evitar que nuestro algoritmo aprenda de estos patrones extraños."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinando features\n",
    "\n",
    "De momento hemos descubierto algunas cosas que pudieran ser interesantes, como por ejemplo,\n",
    "* *Algunos patrones extraños en los datos* que sería interesante investigar (como las líneas en el gráfico de dispersión de *medianHouseValue vs medianIncome*)\n",
    "* *Distribuciones* de nuestras features bastante *asimétricas hacia la derecha*.\n",
    "* **Correlaciones interesantes** entre algunas de nuestras features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precísamente en este último apartado parece que nos permitió ver que entre las features totalRooms, totalBedrooms, households y population existía una relación directa ¿qué ocurre si intentamos combinar algunas de estas features para, con un poco de suerte, obtener otra más explicativa de la variable target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roomsPerHousehold = totalRooms / householdsweighed_medianHouseValue\n",
    "housing[\"roomsPerHousehold\"] = housing[\"totalRooms\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bedroomsPerHousehold = totalBedrooms / households\n",
    "housing[\"bedroomsPerHousehold\"] = housing[\"totalBedrooms\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populationPerHousehold = population / households\n",
    "housing[\"populationPerHousehold\"] = housing[\"population\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bedroomsPerRoom = totalBedrooms / totalRooms\n",
    "housing[\"bedroomsPerRoom\"] = housing[\"totalBedrooms\"] / housing[\"totalRooms\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver si la matriz de correlación arroja algo nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"medianHouseValue\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Bueno! roomsPerHousehold aporta cierta información indicándonos que, como parece lógico, cuanto más habitaciones tiene una casa en teoría más grande será y por tanto más cara.\n",
    "Por otro lado parece que cuanto más habitaciones de una casa están dedicadas a dormitorios, menor es el valor de esta.\n",
    "De todos modos tampoco hay que ser desde el primer momento completamente obsesivo por probar todas las combinaciones posibles, **el objetivo es conseguir conocimiento sobre los datos, montar un prototipo, analizar su salida e ir iterando de esta manera**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de los datos\n",
    "\n",
    "Es hora de adaptar nuestro dataset para ponerle las cosas \"fáciles\" a nuestro modelo, es decir, disponer los datos de una manera que *un algoritmo de Machine Learning* se sienta cómodo con ellos. A la hora de hacer esto es conveniente, en vez de hacerlo manualmente, **invertir tiempo en montar funciones por las siguientes razones**,\n",
    "* Nos permitirán **reutilizar nuestras transformaciones** en cualquier futuro dataset que nos pasen.\n",
    "* Incluso para futuros proyectos, ir **preparando una librería con nuestras funciones** será de gran utilidad.\n",
    "* Además podremos usar estas funciones en nuestro sistema productivo para **integrarlas en el pipeline** que permitirá procesar estos.\n",
    "* El proceso de probar varias transformaciones y finalmente quedarnos con aquellas que nos resulten más útiles será más fácil y rápido cuantas menos manualidades hagamos.\n",
    "\n",
    "Primeramente obtengamos de nuevos nuestros datos limpios y separemos las features de la variable target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"medianHouseValue\", axis=1)\n",
    "housing_labels = strat_train_set[\"medianHouseValue\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Poner las cosas fáciles* puede sonar ambiguo y es que cada modelo es un mundo de manera que las funciones de pérdida (loss functions) que cada uno tiene como objetivo minimizar, son distintas (con el enfoque común de que el objetivo es cometer el menor error posible de acuerdo a ellas). De este modo, no todos los modelos se benefician por igual de todas las técnicas. Una de las más habituales como es la estandarización de features es conveniente en el caso de las regresiones (lineal y logística), k nearest neighbours (KNN), Support Vector Machines (SVMs) y redes neuronales...pero en el caso de los árboles de decisión (y sus ensembles) o modelos bayesianos, no se suele aplicar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores missing, nulos, NaN...\n",
    "\n",
    "Son pocos los modelos que se sienten confortables con **valores ausentes (null, NaN, blancos...)**; como recordaremos, al principio introdujimos adrede unos cuantos NaN en totalBedrooms y ahora toca encargarse de ellos. Para ello tenemos principalmente tres opciones,\n",
    "* **Eliminar** los **distritos** que contengan algún NaN, en este caso para esta feature.\n",
    "* **Eliminar** la **feature** directamente, sin contemplaciones.\n",
    "* \"Rellenar\" esos valores ausentes con algún valor que creamos que pueda ser adecuado (media, mediana, cero...). Este concepto se conoce como **Imputación**\n",
    "\n",
    "Lo bueno es que `pandas` pone a nuestro alcance poderosos métodos para el tratamiento de este tipo de valores en nuestros DataFrame. Para la **primera opción**: `dropna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.dropna(subset=[\"totalBedrooms\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la **segunda opción**: `drop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.drop(\"totalBedrooms\", axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la **tercera opción** (si decidieramos por ejemplo rellenar los NaN con la mediana): `fillna` con el valor que consideremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = housing[\"totalBedrooms\"].median()\n",
    "housing[\"totalBedrooms\"].fillna(median).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si optamos por la opción tres, guardaremos el valor calculado de la mediana (en este caso) como oro en paño, ya que este será el que tengamos que utilizar también para reemplazar aquellos NaN de totalBedrooms en el conjunto de test y también cuando nuestro modelo productivo empiece a recibir datos....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/computerguy.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, **¡comprueba tu versión de Scikit-learn!**, a partir de la versión 0.20.0 se han introducido cambios relevantes en los módulos de preprocesamiento de la librería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scikit-learn version \" + sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **SimpleImputer**, `scikit-learn` le da una vuelta de tuerca al antiguo `Imputer` dando como resultado el `SimpleImputer`. Entre las mejoras realizadas tenemos,\n",
    "    * Posibilidad de reemplazar valores categóricos.\n",
    "    * Imputación de un valor constante (fill value)\n",
    "    \n",
    "  Más información [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto `Imputer` (en su momento) como `SimpleImputer`, actuarán sobre todas las features del DataFrame desde el que se invoque. Esto quiere decir que la estrategia indicada a través del parámetro `strategy` se aplicará a todas las features por igual.\n",
    "Puesto que se ha elegido aplicar la estrategia `median` (mediana), que es una estrategia aplicable de forma exclusiva a features numéricas, crearemos una copia de nuestros datos excluyendo aquellas features no numéricas (ocean_proximity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya podemos entrenar nuestro `SimpleImputer` sobre el dataset que contiene exclusivamente features numéricas, esto lo haremos mediante el método `fit`, este método lo que hace basicamente es \"entrenar\" nuestro `SimpleImputer` antes de aplicarlo a un dataset mediante el método `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si se utiliza notación científica por defecto, es posible cambiar este comportamiento\n",
    "# Si modificamos el parámetro suppress del método set_printoptions de numpy a True\n",
    "# siempre se imprimirán los números en coma flotante en notación decimal\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el `Imputer`/`SimpleImputer` obtiene los mismos resultados que si aplicáramos la mediana \"a pelo\". La gran ventaja del `Imputer`/`SimpleImputer` es que nos cubre en caso de que ya sea en el conjunto de test o más adelante, alguna de nuestras features numéricas tenga algún NaN; él se encargará de tomar todos los NaN sean de la feature numérica que sean y reemplazarlos por la mediana calculada (o lo que sea, según la estrategia configurada)\n",
    "\n",
    "Ahora utilizaremos el `Imputer`/`SimpleImputer` entrenado para reemplazar los NaN existentes por las medianas aprendidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es complicado reconventir este array de `numpy` a un DataFrame de `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing_tr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` proporciona [otras](https://scikit-learn.org/stable/modules/impute.html) clases que plantean métodos de imputación más sofisticados. Aunque al final, el más adecuado lo determinará seguramente nuestro caso de uso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Scikit-Learn: sólidos principios de diseño.</center></h3>\n",
    "<center>API design for machine learning software: experiences from the scikit-learn project (https://arxiv.org/pdf/1309.0238v1.pdf)<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* **Consistencia**, todos los objetos comparten una API simple y consistente.\n",
    "\n",
    "    a) Estimators, cualquier objeto que pueda estimar parámetros en base al dataset es considerado un Estimator (por ejemplo un `SimpleImputer`). La estimación se realiza mediante el método `fit()`, que toma como parámetro el dataset (también puede admitir dos parámetros siendo el segundo la parte del dataset que contiene las labels)\n",
    "    \n",
    "    b) Transformers, algunos Estimators (como el propio `SimpleImputer`), pueden también transformar un dataset. Estos son los Transformers y cuentan con un método llamado `transform()` que se encarga de transformar, en base a los parámetros aprendidos, el dataset que recibe por parámetro devolviéndolo transformado. Cuentan además con un método llamado `fit_transform()` (cuyos resultados son similares a llamar a `fit()` y a `transform()` de manera consecutiva, pero a veces está optimizado para mejorar tiempos de ejecución)\n",
    "    \n",
    "    c) Predictors, existen Estimators que pueden hacer predicciones dado un determinado dataset. Por ejemplo el modelo `LinearRegression` es un Predictor y tiene un método `predict()` que le permite devolver las predicciones para un dataset nuevo que reciba. Además cuenta con un método `score()` que evalua la calidad de las predicciones realizadas comparándolas con el conjunto de etiquetas (en el caso del aprendizaje supervisado).\n",
    "    \n",
    "\n",
    "* **Inspección**, todos los hiperparámetros de un estimador son accesibles mediante variables de la propia instancia (en el caso de `SimpleImputer`, podríamos acceder a la estrategia configurada mediante `SimpleImputer.strategy`) incluso los parámetros aprendidos pueden ser también accedidos como vimos con `SimpleImputer.statistics_` (este tipo de propiedades se caracterizan porque llevan como sufijo un underscore)\n",
    "\n",
    "* **No proliferación de clases**, se respeta la máxima de no reinventar la rueda: los datasets se modelan como DataFrames de `pandas`, arrays de `numpy` o matrices dispersas de `scipy`. En cuanto a los hiperparámetros son strings o números de Python normales y corrientes.\n",
    "\n",
    "* **Composition**, las piezas ofrecidas por `scikit-learn`, se pueden utilizar como bloques de construcción de Pipelines, como se verá más adelante en el curso. ¿Parece interesante? ¿quieres saber más? echa un vistazo a la diapositiva *00_project_Flow.ipynb (Composition)*.\n",
    "\n",
    "* **Defaults razonables**, los valores por defecto configurados para los múltiples parámetros son razonables, es decir, permiten crear un sistema funcional del cual partir, sin grandes quebraderos de cabeza.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gestionando texto y features categóricas (nominales y ordinales)\n",
    "\n",
    "El hecho de que a lo largo de estos pasos hayamos estado marginando el campo **ocean_proximity**, viene dado porque al ser un **atributo no numérico**, hay pocos cálculos que podamos hacer con el (por ejemplo no podríamos calcular la mediana). Además la mayoría de **los algoritmos de Machine Learning trabajan mejor con números que con categorías**. Una clase que nos puede ayudar a la hora de trabajar con features categóricas es,\n",
    "   * **OrdinalEncoder**, recibirá una o más features de tipo entero o de tipo categórico y devolverá una columna de números (su tipo vendrá indicado por el parámetro `dtype`) por cada feature, cuyos valores irán desde *0* hasta el *número de categorías - 1*. Veamos un ejemplo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team = pd.DataFrame({'position':['keeper','defender','defender','defender','midfielder','forward'], \n",
    "                     'seasons':[1,2,2,3,1,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# El parámetro dtype determina el tipo de dato de las \n",
    "# categorías devueltas por el OrdinalEncoder.\n",
    "# Ha de ser numérico (algún subtipo de entero o float)\n",
    "ordEncoder = OrdinalEncoder(dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordEncoder.fit(team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El atributo categories_ identifica las categorías \n",
    "# identificadas por el OrdinalEncoder para cada feature\n",
    "ordEncoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ordEncoder.transform(team)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Cuidado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signing = pd.DataFrame({'position':['manager'], 'seasons':[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Antes de ejecutar...¿Qué crees que ocurrirá? ¿Cómo se comportará el OrdinalEncoder?\n",
    "ordEncoder.transform(signing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más recomendable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El parámetro dtype determina el tipo de dato de las \n",
    "# categorías devueltas por el OrdinalEncoder.\n",
    "# Ha de ser numérico (algún subtipo de entero o float)\n",
    "ordEncoder = OrdinalEncoder(categories=[['keeper','defender','midfielder','forward','manager'], \n",
    "                                        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordEncoder.fit_transform(signing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordEncoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`OrdinalEncoder` permite a través de su propiedad `categories_` listar explícitamente las categorías de nuestras features, eso es de gran ayuda para asegurarnos de que todas las categorías que nos interesan serán contempladas por nuestro transformer ([disponible](https://github.com/scikit-learn/scikit-learn/pull/12367) a partir de la versión 0.20.1 y posteriores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apliquemos el `OrdinalEncoder` a nuestra feature categórica ocean_proximity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing['ocean_proximity'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "housingOrdEncoder = OrdinalEncoder(dtype=np.uint8)\n",
    "housing_cat_encoded = housingOrdEncoder.fit_transform(housing_cat.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housingOrdEncoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing_cat_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **LabelEncoder**, se trata de un transformer que se ideó para codificar el target o label en problemas de clasificación. A cada posible clase (representada por un entero o un string) le asignará un entero entre 0 y el *número de clases - 1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema con este tipo de representación es que existen algoritmos de Machine Learning que pueden asumir que dos categorías asociadas a valores próximos (0 y 1) son más similares entre si que dos categorías asociadas a valores más lejanos (0 y 4) y en nuestro caso esto no nos interesa en absoluto. \n",
    "Para resolver este problema podemos recurrir a la notación **One-Hot Encoding** que consiste en representar cada categoría en una combinación de bits tales que todos serán 0 salvo uno de ellos que será 1 (alternando este entre los distintos valores que componen la categoría).¿One hot Chili Peppers? echa un vistazo a la diapositiva *00_project_Flow.ipynb (One-hot)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat.values.reshape(-1,1))\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es una matriz dispersa (`sparse matrix`) de `scipy`: las matrices dispersas son muy interesantes para aquellas situaciones en las que tenemos una matriz en la que la mayor parte de la información son ceros. En este caso tenemos cerca de 17000 filas de las cuales tan solo una columna contiene un 1 por fila; sería un desperdicio de memoria utilizar un montón de espacio en almacenar ceros, la matriz dispersa soluciona esto almacenando las posiciones de los elementos relevantes (los que no son cero). Se puede convertir en array de `numpy` fácilmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **LabelBinarizer**, como su propio nombre indica permite binarizar los valores de nuestro target o label. En el caso de que solo pueda tomar dos posibles valores (sano/enfermo, ok/ko, sobrevive/fallece...) la binarización convertirá los valores en 1 y 0. Si por el contrario tenemos un problema multiclase (pobre/medio/rico, muy barato/barato/en la media/caro/muy caro...) `LabelBinarizer` transformará los posibles valores de la clase a notación one-hot. El resultado es un array de `numpy`, aunque esto es configurable mediante un parámetro: `sparse_output`, que si se configura a `True`, devolverá una sparse matrix de `scipy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColumnTransformer\n",
    "\n",
    "`Scikit-learn` ya ofrecía una amplia cantidad de transformers que permiten llevar a cabo distintas operaciones sobre nuestros datos. El problema es que, en el mundo real, es sencillo encontrarse datos en los que las distintas features son de diversos tipos, esto implica que los requisitios de preprocesamiento no serán los mismos para todas ellas siendo necesario por tanto aplicar transformaciones de manera focalizada según el tipo de cada feature.\n",
    "Hasta ahora `scikit-learn` no ofrecía una manera simple (out of the box) para esta tarea...hasta la versión 0.20.0 que incluye el `ColumnTransformer`.\n",
    "Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame de juguete, con columnas numéricas y categóricas. En la columna edad hay un NaN\n",
    "equipo = pd.DataFrame({\n",
    "    \"nombre\": np.array([\"Julio\", \"Nuria\", \"Jose\", \"Luis\", \"Daniel\", \"Javier\", \"Alberto\", \"Lourdes\"]),\n",
    "    \"edad\": np.array(  [     22,      26,     28,     25,       np.nan,       24,        24,        26]),\n",
    "    \"sexo\": np.array(  [\"Hombre\", \"Mujer\",\"Hombre\",\"Hombre\",\"Hombre\",\"Hombre\", \"Hombre\",  \"Mujer\" ]),\n",
    "    \"demarcacion\": np.array( [\"portero\", \"defensa\", \"defensa\", \"medio\", \"medio\", \"medio\", \"delantero\", \"defensa\"]),\n",
    "    \"estatura\": np.array( [190, 187, 183, 170, 168, 180, 191, 175])\n",
    "}, columns=[\"nombre\", \"edad\", \"sexo\", \"demarcacion\", \"estatura\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([(\"one_hot\", OneHotEncoder(sparse=False, dtype=np.uint8), [2, 3]),\n",
    "                        (\"imputa_guay\", SimpleImputer(strategy=\"mean\"), [1])], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = ct.fit_transform(equipo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y en el caso de que nos dé igual el poder nombrar cada step del `ColumnTransformer`, podremos recurrir a este atajo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_made = make_column_transformer((OneHotEncoder(sparse=False, dtype=np.uint8), [2, 3]),\n",
    "                                   (SimpleImputer(strategy=\"constant\", fill_value=30), [1]), remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ct_made.fit_transform(equipo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_made.named_transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Transformers personalizados\n",
    "\n",
    "Si bien es cierto que `scikit-Learn` ofrece una gran variedad de transformers, al final llegará el momento en el que tengamos que definir los nuestros propios y queremos que al definirlos, podamos trabajar con ellos de igual manera que lo haríamos con cualquiera de los transformers out of the box (por ejemplo que podamos integrarlo en un pipeline sin ningún problema).\n",
    "\n",
    "Se recomienda utilizar `BaseEstimator` y `TransformerMixing` como clases base, de este modo solo tendremos que implementar dos métodos para nuestra clase transformer: `fit()` y `transform()`. Por ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representamos mediante una clase nuestro transformer cuyo objetivo será realizar la combinación\n",
    "# de features que anteriormente abordamos de forma manual. Además le añadimos un hiperparámetro para \n",
    "# poder elegir si se desea combinar o no las features totalBedrooms y totalRooms.\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, index_list, add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        self.indices = index_list\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "            \n",
    "        room_ix, bedroom_ix, population_ix, household_ix = self.indices\n",
    "        \n",
    "        rooms_per_household = X[:, room_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedroom_ix] / X[:, room_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos el parámetro que hemos establecido\n",
    "# como obligatorio para nuestro constructor\n",
    "featEng_indices = [3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invocamos al constructor como cualquier Transformer out of the box\n",
    "caa = CombinedAttributesAdder(featEng_indices, add_bedrooms_per_room=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con el objeto creado invocamos fit_transform()\n",
    "housFeatEng = caa.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reconvertimos la matriz de numpy a un dataframe para\n",
    "# poder comprobar los resultados\n",
    "pd.DataFrame(housFeatEng, columns=list(housing) + ['rooms_per_household','population_per_household']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La inclusión de hiperparámetros (¡sí! `add_bedrooms_per_room` es un hiperparámetro) en nuestros transformadores custom permite habilitar o deshabilitar de forma sencilla aquellos pasos de preparación de datos sobre cuya efectividad no estamos muy seguros; esto permite probar distintas combinaciones de forma fácil y rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como ya hemos podido comprobar, el procesamiento de features numéricas\n",
    "# y de features categóricas, son diferentes. Es por ello muy posible que\n",
    "# queramos que la parte categórica de nuestro dataframe y la parte numérica\n",
    "# sigan procesamientos diferentes (pipelines diferentes)\n",
    "# Hasta la llegada del ColumnTransformer no había una solución out of the box\n",
    "# para conseguir esto. Sin embargo podíamos hacer la nuestra propia de forma sencilla\n",
    "# Creemos un sencillo transformer que nos ayude\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.feature_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['totalBedrooms','medianIncome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = DataFrameSelector(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df1.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado de features\n",
    "\n",
    "Esta es una parte muy a tener en cuenta ya que los algoritmos de Machine Learning se desenvolverán mejor o peor dependiendo de la manera en la que reciban los datos y un punto que afecta a muchos de ellos es el recibir las features numéricas en escalas radicalmente diferentes entre si, por ejemplo: **totalRooms oscila entre 2 y 39320 mientras que medianIncome va de 0 a 15**. Existen dos formas principales de gestionar este tema,\n",
    "* **Min-Max Scaling**, se trata de ajustar los valores de cada feature de manera que caiga dentro del rango de valores que hemos definido (por defecto entre 0 y 1), esto se consigue aplicando la siguiente operación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "                        X_scaled = X_std * (max - min) + min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde `Scikit-Learn` se proporciona la clase `MinMaxScaler`, que se encarga de realizar este tipo de escalado sobre cada feature de manera que sus valores se ciñan al rango deseado (el cual es además configurable a través del hiperparámetro `feature_range`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Standardization**, responde a la predilección que sienten los algoritmos de Machine Learning por aquellos datos distribuídos normalmente (media 0 varianza 1); basicamente para cada feature calcula la diferencia entre cada valor y la media y divide el resultado entre la varianza. El punto fuerte de esta técnica es que, al no tener un máximo y un mínimo establecidos, es menos sensible a outliers: si por ejemplo tuvieramos un medianIncome que en vez de estar en el rango 0-15 se fuera hasta 100, un `MinMaxScaler` apelotonaría todos los valores de medianIncome entre 0 y 0.15 siendo el 1 el outlier con 100.\n",
    "\n",
    "A través del `StandardScaler`, `Scikit-learn` implementa esta técnica.\n",
    "<img src=\"images/standardization.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta dos detalles importantes, \n",
    "* El escalado se aplica basicamente a las features; rara vez es requerido en la variable target.\n",
    "* Los Scaler requieren previamente a la invocación de su método `transform()`, que se aplique su método `fit()` sólamente sobre el conjunto de datos de training (lo de siempre, el conjunto de test, ni tocarlo), una vez hecho esto ya se podrá aplicar el método `transform()` sobre el propio conjunto de training, el de test y los nuevos datos.\n",
    "* Al igual que con los métodos de imputación, no solo existen los aquí citados. Podemos ver más de los disponibles en `Scikit-learn` [aquí](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS\n",
    "¡Atención! por haber llegado hasta aquí, os merecéis un bonus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas Profiling\n",
    "Habéis descubierto una librería que lleva varios niveles más allá la información dada por el método `describe()` de `pandas`. Genera un informe en HTML y CSS3 que aporta detallada información estadística y gráfica para cada variable.\n",
    "* Para instalar `pandas profiling` podemos hacerlo mediante\n",
    "    * `conda install pandas-profiling` si queremos usar el conda package manager.\n",
    "    * `pip install pandas-profiling` si queremos usar el pip package manager.\n",
    "* Una vez instalado podremos importarlo en nuestro notebook mediante `import pandas_profiling`\n",
    "* Para poder generar un reporte con `pandas profiling`, tendremos que haber cargado primeramente nuestros datos\n",
    "en un DataFrame de `pandas`.\n",
    "* Una vez tenemos nuestros datos en un DataFrame ejecutaremos `pandas_profiling.ProfileReport(df)` y el reporte se mostrará en nuestro notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al mismo tiempo `pandas profiling` descarta por defecto aquellas features que estén áltamente correladas (hablamos de un umbral superior por defecto a 0.9) con otras features ya existentes, aunque brinda opciones para desactivar el chequeo de correlaciones, cambiar el umbral e incluso proporcionar una lista de variables que no serán rechazadas aunque no superen el chequeo de correlación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como no, es posible además guardar en disco el reporte generado, para ello recogeremos el resultado de ejecutar el ProfileReport: \n",
    "`pR = pandas_profiling.ProfileReport(df)` y seguídamente lo almacenaremos en la ruta deseada `pR.to_file(outputfile=\"myoutputfile.html\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "ownership": {
   "author": "Alejandro Manuel Arranz Lopez",
   "company": "datahack",
   "copyright": "datahack all rights reserved"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
