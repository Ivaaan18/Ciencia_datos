{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio: web scraping con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio vas a hacer [scraping](https://es.wikipedia.org/wiki/Web_scraping) de unas cuantas páginas de productos de [Amazon España](https://www.amazon.es/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El scraping consiste en utilizar una herramienta informática (como puede ser un lenguaje de programación) para extraer datos de una página web de forma automática. Básicamente utiliza [peticiones HTTP](https://es.wikipedia.org/wiki/Hypertext_Transfer_Protocol) para \"pedir\" una página web de forma similar a como haríamos con un navegador web (como Firefox, Chrome o Internet Explorer). Una vez hecha dicha petición, extrae la información que nos interesa y la guarda (en un archivo o en una base de datos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que necesitamos es una biblioteca capaz de hacer estas peticiones HTTP, es decir: que se conecte a la página web que queremos, y nos \"traiga\" a Python el contenido de dicha web.\n",
    "\n",
    "Para este ejercicio vamos a utilizar la página web de Amazon de esta bicicleta de montaña: https://www.amazon.es/Moma-Bicicleta-Mountainbike-aluminio-suspensi%C3%B3n/dp/B00VXE2JCY/ref=lp_2929469031_1_1?s=sports&ie=UTF8&qid=1465237289&sr=1-1&th=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pide HTTP con `Requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python incluye en la biblioteca estándar algunas utilidades para hacer peticiones HTTP. Un ejemplo es [`urllib`](https://docs.python.org/3/library/urllib.html). No obstante, se trata de una biblioteca bastante complicada para algo tan sencillo como pedir páginas web. \n",
    "\n",
    "Por eso mismo vamos a utilizar una biblioteca mucho más sencilla y potente: [`requests`](http://docs.python-requests.org/en/master/). Requests no es parte de la bilbioteca estándar, sino que debe ser instalada. No obstante, la distribución de Python Anaconda (la que utilizamos para este curso) la trae preinstalada; así que no tenemos que hacerlo.\n",
    "\n",
    "De todos modos es muy fácil de instalar. No tendríamos más que hacer:\n",
    "\n",
    "```sh\n",
    "$ pip install requests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La [documentación de Requests](http://docs.python-requests.org/en/master/#the-user-guide) es explícita, liviana y entretenida de leer. Sin más dilación, importemos la biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa aquí la bilbioteca requests:\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez importada, Requests debe \"pedir\" una página web (en este caso https://www.amazon.es/Moma-Bicicleta-Mountainbike-aluminio-suspensi%C3%B3n/dp/B00VXE2JCY/ref=lp_2929469031_1_1?s=sports&ie=UTF8&qid=1465237289&sr=1-1&th=1). Solo tenemos que decirle a Requests que haga lo que se llama *una petición HTTP GET*. Los dos tipos básicos de peticiones en web son las [HTTP GET y las HTTP POST](http://www.w3schools.com/tags/ref_httpmethods.asp). En este caso, puesto que no queremos \"subir\" nada, sino recibir, haremos una HTTP GET con Requests.\n",
    "\n",
    "Además, ciertas peticiones deben llevar algo de información adicional, lo que se conoce como [cabeceras](https://es.wikipedia.org/wiki/Cabeceras_HTTP) o *headers*, que son pares clave-valor. Para el caso de Amazon España, necesitamos decirle a sus servidores web que *somos un navegador normal y corriente, y no la biblioteca `requests` de Python*. Si no lo hacemos, Amazon nos bloqueará el contenido :(\n",
    "\n",
    "Para ello, necesitamos añadir una cabecera que sea de la siguiente forma: como clave `\"User-Agent\"`, y como valor `\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:56.0) Gecko/20100101 Firefox/56.0\"` (para *hacernos pasar* por el Firefox de un Mac).\n",
    "\n",
    "*Importante:* A fecha del 9 de septiembre de 2017, Amazon ha estado trabajando duro durante el verano y ha mejorado bastante su modelo de machine learning de identificación de bots y scrappers (antes era una patata y muy fácil de engañar). Si crees que estás haciendo bien el ejercicio, pero que no consigues extraer el texto (y que en los datos extraídos salen cosas raras como *Teclea los caracteres que aparecen en la imagen*), es muy posible que Amazon crea (correctamente) que le estamos intentando scrapear. Hay varias técnicas para intentar engañarle: apagar y encender el wifi en el portátil (para re-conectarnos a Internet), ir cambiando el *User Agent* que utilizamos (en vez del `\"Firefox/56.0\"` podemos probar `\"Firefox/55.0\"`, `\"Chrome/51.0.2704.103\"`, `\"PepitoDeLosPalotes\"`... Cualquier cosa vale), copiarte el Notebook y ejecutarlo desde otro ordenador o red... Si ves que te da muchísimos problemas y que no consigues que Amazon te bendiga con su visto bueno, no dudes en escribirme.\n",
    "\n",
    "En la [quickstart](http://docs.python-requests.org/en/master/user/quickstart/#make-a-request) puedes informarte sobre lo sencillo que es hacer una petición GET con Requests. También tendrás que mirar cómo añadir tu *header* a la petición. Al lío:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haz una petición GET de la url de la bicicleta, con el \n",
    "# header especificado, y asigna el resultado en una variable \n",
    "# llamada peticion_bicicleta:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver el tipo de nuestra variable `peticion_bicicleta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime con un print el tipo de dato de\n",
    "# la variable peticion_bicicleta:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un tipo de dato (una instancia) de la clase [`Response`](http://docs.python-requests.org/en/master/api/#requests.Response). \n",
    "\n",
    "Cuando intentamos ver una página web (o dicho de otro modo: hacemos una petición HTTP a dicha página), el servidor de dicha web nos devuelve un [código HTTP de respuesta](https://es.wikipedia.org/wiki/Anexo:C%C3%B3digos_de_estado_HTTP). Toca buscar en la documentación si las instancias de la clase `Response` tienen algún atributo (*class*/*instance variable*) o método para saber si la petición era adecuada, y si el contenido nos ha sido devuelto correctamente..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime con un print el código HTTP de \n",
    "# respuesta o Status de peticion_bicicleta:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora puedes mirar [aquí](https://es.wikipedia.org/wiki/Anexo:C%C3%B3digos_de_estado_HTTP) si el código HTTP de respuesta significa algo bueno o algo malo (es decir: si todo ha salido bien o no).\n",
    "\n",
    "Si ha salido bien, podemos seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno: resulta que una página web no deja de ser código. A pesar de que nosotros vemos la web de forma bonita y llena de imágenes y texto legible, las páginas están escritas en principalmente tres lenguajes: [HTML](https://es.wikipedia.org/wiki/HTML), [CSS](https://es.wikipedia.org/wiki/Hoja_de_estilos_en_cascada) y [Javascript](https://es.wikipedia.org/wiki/JavaScript).\n",
    "\n",
    "Cuando pedimos una página web con nuestro navegador, éste recibe el código de la página. Una vez recibido, el nevagador \"dibuja\" o *renderiza* ese código para mostrarnos las páginas web bonitas y radiantes.\n",
    "\n",
    "`Requests` no hace este útlimo paso: únicamente recibe el código de la página (llamado *código fuente*), con el que vamos a interactuar para hacer nuestro scraping.\n",
    "\n",
    "Como prueba, vamos a hacer una cosa: vamos a abrir con nuestro navegador web favorito la [página de la bici](https://www.amazon.es/Moma-Bicicleta-Mountainbike-aluminio-suspensi%C3%B3n/dp/B00VXE2JCY/ref=lp_2929469031_1_1?s=sports&ie=UTF8&qid=1465237289&sr=1-1) y vamos a hacer click derecho, por ejemplo, en el título del artículo (*Bicicleta Montaña Mountainbike 26\" BTT SHIMANO, aluminio, doble disco y doble suspensión*), y hacemos click en *Inspeccionar elemento*. Debería desplegarse una ventanita donde aparece el código fuente (en HTML principalmente) de esa parte de la página web.\n",
    "\n",
    "Asimismo, algunos navegadores web nos permiten ver todo el código fuente de la página web en la que estamos: haciendo click derecho en cualquier sitio de ésta es posible que aparezca una opción llamada *Ver código fuente de la página* o algo similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Requests` recibe el código fuente en lo que se llama *el contenido de la respuesta* tras la petición HTTP GET. Igual está disponible con algún atributo/método en `peticion_bicicleta`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imprime el código fuente de la página web\n",
    "# de la bici. Va a ser largo, pero no pasa nada.\n",
    "# Es posible que aparezca muchísimo espacio en\n",
    "# blanco, pero no pasa nada: el código fuente\n",
    "# de la página es el que es.\n",
    "# Para ello, crea una nueva variable llamada\n",
    "# codigo_fuente_bicicleta, y asígnale el \n",
    "# valor del código fuente recogido por Requests:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahí está todo: todo el texto, todas las imágenes... Todo. De ahí ahora seleccionaremos las partes que nos interesan, que van a ser los comentarios del producto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preciosa sopa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver el tipo de dato que es `codigo_fuente_bicicleta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haz un print del tipo de dato de\n",
    "# codigo_fuente_bicicleta:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un string de Python normal y corriente. Eso está bien: probablemente sea fácil extraer la información que queremos de ese caos de código HTML, CSS y Javascript..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Sí lo es. Afortunadamente, existe una biblioteca muy famosa para hacer web scraping llamada [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). `BeautifulSoup` cogerá este código fuente y nos permitirá extraer lo que necesitamos de forma sencilla.\n",
    "\n",
    "`BeautifulSoup` es una biblioteca de terceros, así que será necesario instalarla. No obstante, de nuevo: Anaconda la trae instalada para nosostros. Así que podemos empezar a usarla directamente. Para ver cómo importarla igual te viene bien mirar un poco el [*quickstart*](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start) que los desarrolladores de la biblioteca han preparado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que `BeautifulSoup` sea capaz de \"analizar por nosotros\" el código de la web es necesario crear una instancia de la clase `BeautifulSoup`. Dicha instancia debe ser creada con dos argumentos posicionales:\n",
    "\n",
    "1. El primero deber ser el código fuente de la web en string (¡lo tenemos!)\n",
    "2. El segundo ha de ser un string que le diga a `BeautifulSoup` el *parser* (o procesador) a utilizar. La biblioteca puede utilizar [unos cuantos](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser). Nosotros utilizaremos el `\"lxml\"`.\n",
    "\n",
    "Manos a la obra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una instancia de BeautifulSoup\n",
    "# llamada bici_bs:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bici_bs` es una instancia de `BeautifulSoup`, lista para \"darnos lo que le pidamos\" de todo el código fuente de la página web.\n",
    "\n",
    "Ahora solo queda decirle qué queremos...\n",
    "\n",
    "Esta parte es más complicada si no sabes HTML, así que iremos despacio:\n",
    "\n",
    "HTML es un lenguaje de marcas. Los elementos que podemos ver en una página web (como son títulos, párrafos, imágenes...) tienen nombres definidos. Por ejemplo:\n",
    "\n",
    "+ Los títulos grandes son `h1`\n",
    "+ Un párrafo es `p`\n",
    "+ Una imagen es `img`\n",
    "\n",
    "La tónica es casi siempre la misma: en HTML debemos \"meter\" el contenido de un elemento entre *marcas*. Por ejemplo, para hacer un título sería:\n",
    "\n",
    "```html\n",
    "<h1>Esto es un título.</h1>\n",
    "```\n",
    "\n",
    "Ponemos `<h1>` al principio, para indicar que comienza un título; y `</h1>` al final para decir que ahí termina. Otro elemento común son los `<div>`, que permiten \"agrupar\" varios elementos en un bloque. Por ejemplo:\n",
    "\n",
    "```html\n",
    "<div>\n",
    "    <h1>Esto es un título</h1>\n",
    "    <p>Y esto un párrafo. Y ambos están dentro de un div</p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "A los señores desarrolladores de la web de Amazon España les gusta mucho utilizar otro elemento de HTML llamado `<span>`, que es similar a `<p>`. Lo veremos cuando analicemos el código fuente de los comentarios de la bici.\n",
    "\n",
    "Los elementos de HTML pueden tener dos cosas llamadas `id` y `class`. Esto permite a los propios desarrolladores poner algo de orden dentro de lo que es el caos de las páginas web modernas. Por ejemplo: una página como la de la bici tiene cientos de `<span>` y párrafos...\n",
    "\n",
    "Pues bien, cada elemento puede (o debería al menos) tener un `id` único si queremos que sea diferenciable del resto de elementos del mismo tipo. Gracias a esto, podemos diferenciar (tanto nosotros como los ordenadores):\n",
    "\n",
    "```html\n",
    "<p id=\"comentario_1\">Primer comentario</p>\n",
    "```\n",
    "\n",
    "De:\n",
    "\n",
    "```html\n",
    "<p id=\"comentario_2\">Otro comentario</p>\n",
    "```\n",
    "\n",
    "Asimismo, elementos \"similares\" o del mismo estilo/aspecto suelen tener una misma `class` (no confundir con las clases de Python):\n",
    "\n",
    "```html\n",
    "<h1 class=\"titulo_grande\">Un título</h1>\n",
    "```\n",
    "Y: \n",
    "\n",
    "```html\n",
    "<h1 class=\"titulo_grande\">Otro título</h1>\n",
    "```\n",
    "\n",
    "Pueden ser seleccionados fácilmente diciéndole a `BeautifulSoup`: *quiero todos los elementos de tipo `h1` que sean de `class=\"título_grande\"`*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a nuestro navegador favorito a \"analizar a mano\" un poco del código HTML de la página de la bici. Nos vamos a la sección de *Principales opiniones de clientes*, y hacemos click derecho justo en esa frase. Cuando se despliegue, hacemos click en *Inspeccionar elemento*.\n",
    "\n",
    "En la consolita que se abre, podremos ver que ese *Principales opiniones de clientes* es un elemento de HTML tal que:\n",
    "\n",
    "```html\n",
    "<h3 class=\"a-spacing-small\">Principales opiniones de clientes</h3>\n",
    "```\n",
    "\n",
    "Es un título de tamaño medianito (`h3` es más pequeñito que `h1`), y de `class` `\"a-spacing-small\"`. No sabemos muy bien lo que quiere decir ese nombre de clase, pero su uso tendrá para los desarrolladores de la web...\n",
    "\n",
    "Si miramos el elemento justo debajo de ese `h3` viene ya lo interesante: \n",
    "\n",
    "```html\n",
    "<div class=\"a-row a-spacing-large\">\n",
    "```\n",
    "\n",
    "Si nuestro navegador nos \"subraya\" qué es cada elemento, podrás ver que ese `div` engloba todos los comentarios; que es lo que buscamos... ¡Bingo!\n",
    "\n",
    "Me pregunto si será también así en la página de otros productos de Amazon... Vamos a ver en otra al azar: por ejemplo [esta](https://www.amazon.es/WD-Elements-Disco-externo-port%C3%A1til/dp/B00CRZ2PRM/ref=lp_937903031_1_2?s=computers&ie=UTF8&qid=1465243368&sr=1-2).\n",
    "\n",
    "Pues también. Exactamente igual. `<div class=\"a-row a-spacing-large\">` parece ser constante en los comentarios de todos los productos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosotros ya hemos trabajado. Ahora es el turno de BeautifulSoup. Resulta que cualquier instancia de `BeautifulSoup` tiene el método [`.find()`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find) que permite buscar cualquier tipo de elemento HTML, y se queda con la primera ocurrencia. Este método toma como argumentos interesantes:\n",
    "\n",
    "1. Un posicional, que es un string con el nombre del elemento a buscar (como pueden ser `\"h1\"`, `\"p\"` o `\"div\"`).\n",
    "2. Un argumento opcional llamado `id`, que casualmente permite especificar más en la búsqueda; buscando solo elementos con el `id` especificado como string.\n",
    "3. Otro argumento opcional llamado `class_` (así con barra baja para diferenciarlo de la keyword `class` que sirve para definir clases en Python). Al igual que `id`, permite limitar la búsqueda a los elementos del HTML que tengan dicha `class`.\n",
    "\n",
    "Por ejemplo: si queremos buscar el primer elemento `h2` que tenga `id=\"123\"` y `class=\"front-title-spacing\"`, haríamos:\n",
    "\n",
    "```python\n",
    "mi_instancia_de_BeatutifulSoup.find(\"h2\", id=\"123\", class_=\"front-title-spacing\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos pues a buscar:\n",
    "\n",
    "```html\n",
    "   \n",
    "<div class=\"a-section review-views celwidget\">\n",
    "```\n",
    "\n",
    "En nuestra instancia `bici_bs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza el método .find() para \n",
    "# buscar el elemento de HTML que buscamos,\n",
    "# que es el div que contiene los comentarios\n",
    "# del producto. Guarda el resultado en una\n",
    "# variable llamada bici_comentarios:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué tipo de dato será `bici_comentarios`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime el tipo de dato de\n",
    "# bici_comentarios:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un `Tag` de `BeautifulSoup`. Básicamente: la versión ya filtrada del código fuente de la página, que solo contiene la parte que queremos.\n",
    "\n",
    "Vamos a hacer un `print()` de `bici_comentarios`; a ver qué sale..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo el contenido de ese `div`, de principio a fin. Hemos filtrado mucho; pero aún nos queda para tener los datos que nos interesan.\n",
    "\n",
    "En cada uno de los comentarios queremos quedarnos con:\n",
    "\n",
    "+ El número de estrellas que el usuario le ha dado a la bici.\n",
    "+ El texto del comentario como tal. \n",
    "\n",
    "En este ejercicio vamos a ignorar el resto de información (nombre del autor del comentario, fecha y demás). ¡Siéntete libre de intentar extraerlos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtenerlos nos va a tocar volver a inspeccionar manualmente la página web. Cogemos nuestro navegador web de nuevo, y miramos la línea justo debajo de la del `div` que contiene todas las reviews (la que hemos extraído). Resulta que esta línea es:\n",
    "\n",
    "\n",
    "```html\n",
    "<div id=\"R2L4SORBKQBFYW\" class=\"a-section review aok-relative\">\n",
    "```\n",
    "\n",
    "Y contiene toda la primera review. \n",
    "\n",
    "Tras un poco más de exploración más hacia abajo en el HTML, veremos que la segunda review empieza así:\n",
    "\n",
    "```html\n",
    "<div id=\"R170P7SFNFRTQK\" class=\"a-section review aok-relative\">\n",
    "```\n",
    "\n",
    "Y la tercera:\n",
    "\n",
    "```html\n",
    "<div id=\"R1WXYTOUKC8OJ8\" class=\"a-section review aok-relative\">\n",
    "```\n",
    "\n",
    "Podemos ver un patrón claro:\n",
    "\n",
    "\n",
    "+ La `class` de cada uno es siempre la misma: `\"a-section review aok-relative\"`\n",
    "\n",
    "Utilizar este patrón debería ser sencillo. Si sale bien, podremos obtener todas las reviews ya de forma individual (separadas unas de otras)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de `.find()` (que solo busca la primera ocurrencia que encaje), las instancias tanto de `BeautifulSoup` como de `Tag` tienen el método `.find_all()`. Este método funciona exactamente igual que `.find()`, pero con la diferencia de que busca todos los elementos que satisfacen la búsqueda; y los devuelve en una lista de Python.\n",
    "\n",
    "Probemos entonces a buscar en función del segundo patrón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza el método .find_all() sobre\n",
    "# bici_comentarios para obtener una lista\n",
    "# con todos div con un comentario cada uno,\n",
    "# y llama a dicha lista bici_comentarios_lista:\n",
    "<tu-codigo-aqui>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La página de la bici tiene 8 comentarios (como podrás ver en tu navegador). Dependiendo de cuándo hagas la práctica, es posible que tenga más (o menos, si alguien ha borrado alguno) ¿Qué longitud tiene nuestra `bici_comentarios_lista`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprueba la longitud de \n",
    "# bici_comentarios_lista:\n",
    "<tu-codigo-aqui>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buena señal. Pero aún tenemos que filtrar más en cada uno de esos comentarios...\n",
    "\n",
    "Misma historia. Toca hacer trabajo manual. Queremos extraer el número de estrellas y el cuerpo del comentario. Comencemos por las estrellas.\n",
    "\n",
    "Vamos al navegador y hacemos click derecho en las estrellitas, e inspeccionamos elemento. Veremos que por ahí pone:\n",
    "\n",
    "```html\n",
    "<span class=\"a-icon-alt\">5,0 de 5 estrellas</span>\n",
    "```\n",
    "\n",
    "Y si miramos otras reviews, veremos que el patrón es constante: podemos sacar ese texto buscando elementos de tipo `span` con `class=a-icon-alt`. De nuevo, con el método `.find()` debería bastar. Eso sí: tenemos que hacerlo en cada uno de los comentarios. \n",
    "\n",
    "¿Aplicar la misma acción o transformación a todos los elementos de una colección/iterable? Me suena eso..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quizás puedas usar una función map() o\n",
    "# una list comprehension para extraer ese \n",
    "# elemento de nuestras reviews, que están en\n",
    "# la lista bici_comentarios_lista... O igual un for...\n",
    "# Mete el resultado en una variable que se llame\n",
    "# estrellas_bici_lista:\n",
    "<tu-codigo-aqui>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos a imprimir `estrellas_bici_lista`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime estrellas_bici_lista:\n",
    "<tu-codigo-aqui>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos las estrellas. Pero nos sobra cosa ahí: queremos quedarnos únicamente con las estrellas, como números `float` (porque así lo deseo). Todo el demás texto nos sobra.\n",
    "\n",
    "Una vez que hemos aislado con `BeautifulSoup` el elemento que queremos, podemos utilizar el método `.get_text(\" \")` para quedarnos con el texto en bruto (le pasamos como argumento un string con un espacio en blanco para que nos separe bien los párrafos). De nuevo, queremos hacerlo para las estrellas de cada una de las reviews..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica .get_text() en todos los elementos de\n",
    "# estrellas_bici_lista, y el resultado guárdalo\n",
    "# en una variable llamada estrellas_bici_lista_solotexto:\n",
    "<tu-codigo-aqui>\n",
    "\n",
    "# E imprime dicha variable:\n",
    "<tu-codigo-aqui>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casi las tenemos. Solo nos queda quitar el `\"de 5 estrellas\"` y sustituir la coma por punto. Hacer slicing de cada string, y sustituir coma por punto podría ser la solución. Con hacer un slice desde el inicio hasta el cuarto caracter + un `.replace` debería valer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica una transformación en \n",
    "# cada elemento de estrellas_bici_lista_solotexto,\n",
    "# que se haga el slicing adecuado y reemplazo de coma\n",
    "# por punto para que se quede\n",
    "# solo con el número (por ejemplo: \"5.0\").\n",
    "# Asigna el resultado a una variable que se llame\n",
    "# numero_estrellas_bici_lista:\n",
    "<tu-codigo-aqui>\n",
    "\n",
    "# E imprime el resultado:\n",
    "<tu-codigo-aqui>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos las estrellas. Gracias a que las listas de Python son colecciones ordenadas, sabemos que el primer número de estrellas es del primer comentario, el segundo del segundo, etcétera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, misma historia para el cuerpo del texto de cada comentario. Analizar manualmente, extraer el patrón que permita a `BeautifulSoup` buscar lo que queremos, y aplicar los `.find`, `.find_all`, `.get_text()` y demás transformaciones necesarias para extraer el texto.\n",
    "\n",
    "Recuerda que en `bici_comentarios_lista` tenemos ya una lista con el cuerpo entero de todo lo que hay en cada comentario. Queda filtrar de nuevo a partir de ahí; solo que buscando el texto de cada comentario y no las estrellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¡Tu turno! Consigue quedarte con el\n",
    "# texto de cada review. Utiliza tantas\n",
    "# líneas de código como necesites.\n",
    "# Si ves saltos de línea \\n en tu resultado\n",
    "# no te preocupes: luego los limpiaremos.\n",
    "# Guarda el resultado en una lista (que debería\n",
    "# tener 8 elementos) llamada texto_comentarios_bici_lista:\n",
    "\n",
    "<tu-codigo-aqui>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime texto_comentarios_bici_lista:\n",
    "<tu-codigo-aqui>\n",
    "\n",
    "# E imprime su longitud:\n",
    "<tu-codigo-aqui>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a quitar los posibles `\"\\n\"` de salto de línea que pueda haber. A veces salen al principio y al final de cada comentario. Resulta que las strings tienen el método [`.strip()`](https://docs.python.org/3/library/stdtypes.html#str.strip) para estos propósitos :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica .strip() a cada comentario\n",
    "# de texto_comentarios_bici_lista,\n",
    "# y guarda el resultado en una variable\n",
    "# llamada texto_comentarios_bici_lista_limpio:\n",
    "<tu-codigo-aqui>\n",
    "\n",
    "# E imprime el resultado:\n",
    "<tu-codigo-aqui>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Bien! Tenemos nuestros resultados en dos listas: en una de ellas las estrellas, y en la otra los comentarios. Molaría ahora \"unir\" ambas listas, teniendo en cuenta que el primer elemento de cada una de ellas es el del primer comentario, el segundo del segundo comentario... \n",
    "\n",
    "El objetivo ahora es conseguir una sola lista, donde cada elemento es una tupla que contiene las estrellas y el texto de cada review. ¿Qué función vista en la teoría podríamos usar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une ambas listas en una sola,\n",
    "# que sea una lista de tuplas estilo\n",
    "# (estrellas, comentario). Guarda el\n",
    "# resultado en una variable llamada\n",
    "# estrellas_y_comentarios_bici:\n",
    "<tu-codigo-aqui>\n",
    "\n",
    "# E imprime el resultado:\n",
    "<tu-codigo-aqui>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La vida es más sencilla si metes el código en funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y terminamos. Hemos escrito bastante código para *scrapear* los comentarios y las estrellas de cada review. Estaría bien definir una función que automatizara el proceso. Con un poco de suerte, permitiría scrapear y obtener los resultados directamente de cualquier producto de Amazon España...\n",
    "\n",
    "Así que vamos a definir una función llamada `scrapear_pagina_amazon`, que toma como argumento posicional `link` (que será el string del link de una página de producto de Amazon), y que devuelve una lista con los pares de estrella-comentario; tal y como hemos hecho para la bici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¡Adelante! Siéntete libre de reutilizar\n",
    "# tanto código del que has ido escribiendo\n",
    "# hasta ahora como desees (aunque igual cambiando\n",
    "# el nombre de las variables para que no sea\n",
    "# siempre bici_esto, bici_lo_otro):\n",
    "<tu-codigo-aqui>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba tu función en el link de las bicis (para ver si el resultado es el mismo que obtuviste sin la función):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¡Llama a la función y mira si el resultado es el mismo!\n",
    "<tu-codigo-aqui>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos construido una función que scrapea páginas web de Amazon. Siéntete libre de probar un par más de páginas de productos (que tengan comentarios), a ver si también funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama aquí a la función pasando como argumento otro link de otro\n",
    "# producto de Amazon España:\n",
    "<tu-codigo-aqui>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y con otro producto distinto más:\n",
    "<tu-codigo-aqui>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El scraping web es una herramienta poderosa (y tediosa; todo sea dicho, debido a que páginas como Amazon intentan protegerse de él) para obtener datos de Internet cuando no están disponibles en bases de datos o APIs fáciles de usar. De hecho [hay empresas que se dedican exclusivamente a esto](http://scrapinghub.com/); y las herramientas que utilizan son exactamente las mismas que has utilizado en este ejercicio (`Requests` y `BeautifulSoup`); aunque hay una más...\n",
    "\n",
    "[`Scrapy`](http://scrapy.org/) es un scraper igual que `BeautifulSoup`, pero también es un [*web crawler*](https://en.wikipedia.org/wiki/Web_crawler) o *web spider*. \n",
    "\n",
    "Un *crawler* es un programa que no solo extrae aquello que queremos de una página (scraping), sino que también se encarga de buscar las propias páginas. Por ejemplo: a un crawler escrito con `Scrapy` le podríamos pasar un dominio (como puede ser Amazon.es), y él solo se encargaría de \"meterse\" en todas las páginas de productos de Amazon, utilizando luego `BeautifulSoup` para *scrapear* y extraer los datos que queremos de cada una de ellas.\n",
    "\n",
    "`Scrapy` es una biblioteca algo más complicada. Pero teniendo su [documentación](http://doc.scrapy.org/en/latest/) y algo de paciencia, no deberías tener demasiados problemas para conseguir tus propósitos :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
